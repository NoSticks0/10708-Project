{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pysrc.constants import datapath, N_ITEMS, N_USERS\n",
    "from pysrc.constants import cachepath, datapath, chartpath\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items\n",
    "\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            test_dict[user_id] = items\n",
    "        else:\n",
    "            test_dict[split[0]] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for key in train_dict:\n",
    "    train.append([int(n) for n in train_dict[key].replace('\\n', '').split(' ')])\n",
    "\n",
    "for key in test_dict:\n",
    "    if len(test_dict[key]) == 0:\n",
    "        test.append([])\n",
    "    else:\n",
    "        test.append([int(n) for n in test_dict[key].replace('\\n', '').split(' ')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6909e-02, 5.5961e-01, 6.1204e-01, 1.2375e-01, 6.4387e-01],\n",
       "        [1.4926e-01, 8.4658e-01, 3.4117e-01, 5.0902e-01, 4.2932e-01],\n",
       "        [2.3764e-01, 8.9140e-01, 8.2256e-02, 7.9993e-01, 3.2401e-01],\n",
       "        [1.3064e-01, 8.6999e-01, 5.1305e-01, 3.7245e-01, 8.2512e-01],\n",
       "        [7.3230e-01, 6.6697e-01, 5.1394e-01, 5.0292e-01, 1.9038e-01],\n",
       "        [2.5962e-01, 4.4922e-02, 4.4542e-01, 4.6179e-01, 2.5249e-04],\n",
       "        [7.6863e-01, 8.5990e-01, 7.7411e-01, 3.2251e-01, 7.1687e-01],\n",
       "        [4.3878e-01, 8.9505e-02, 8.5332e-01, 8.6104e-02, 7.4991e-01],\n",
       "        [7.8328e-01, 8.7825e-01, 9.8610e-01, 4.3096e-01, 1.4484e-01],\n",
       "        [4.9623e-01, 4.7774e-01, 5.5073e-01, 8.1089e-01, 9.7849e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((10,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def log_likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(torch.log(yhat+1e-6) * y, axis = 1))\n",
    "\n",
    "def likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(yhat * y, axis = 1))\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims = None,\n",
    "                 kl_weight = .2\n",
    "                 ):\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        \n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [512, 128]\n",
    "            \n",
    "        self.hidden_dims = hidden_dims\n",
    "            \n",
    "        modules.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        modules.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        modules.append(nn.LeakyReLU())\n",
    "        \n",
    "        # Build Encoder\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1])\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(nn.Linear(hidden_dims[-1], input_dim),\n",
    "                                         nn.Sigmoid()\n",
    "                                        )\n",
    "        \n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, self.hidden_dims[0])\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        result = F.normalize(result, p=1)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [F.normalize(self.decode(z), 1, dim=1), x, mu, log_var]\n",
    "\n",
    "    def loss_function(self, recons, x, mu, log_var) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        kld_weight = self.kl_weight\n",
    "        recons_loss = log_likelihood_loss(x, recons)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dim = 91599\n",
    "np.random.seed(0)\n",
    "test_ids = np.random.choice(np.array(list(train_dict.keys())).astype(int), int(.2*len(train_dict.keys())), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_uniform(data, p_relative = .1):\n",
    "    current_sparse = None\n",
    "    batch_size = 1000\n",
    "    X = []\n",
    "    for row in data:\n",
    "        X.append(torch.zeros(item_dim).bool())\n",
    "        for item in row:\n",
    "            if np.random.random() > p_relative:\n",
    "                X[-1][item] = 1\n",
    "        X[-1] = X[-1]\n",
    "    X = torch.stack(X)\n",
    "    return X\n",
    "    \n",
    "def sparsify_items(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def sparsify_users(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def list_batch_to_ohe(data):\n",
    "    return sparsify_uniform(data, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, val, n_epochs = 100):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loading_time = 0\n",
    "        for batch_X, batch_Y in train:\n",
    "            batch_X = batch_X.float()\n",
    "            batch_Y = batch_Y.float()\n",
    "            recons, x, mu, log_var = model.forward(batch_X)\n",
    "            train_loss = model.loss_function(recons, batch_Y, mu, log_var)['loss']\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Done Epoch {epoch}\")\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                total_train_loss = 0\n",
    "                r_train_loss = 0\n",
    "                kl_train_loss = 0\n",
    "                train_batches = 0\n",
    "                for batch_X, batch_Y in train:\n",
    "                    batch_X = batch_X.float()\n",
    "                    batch_Y = batch_Y.float()\n",
    "                    recons, x, mu, log_var = model.forward(batch_X)\n",
    "                    loss = model.loss_function(recons, batch_Y, mu, log_var)\n",
    "                    total_train_loss += loss['loss']\n",
    "                    r_train_loss += loss['Reconstruction_Loss']\n",
    "                    kl_train_loss += loss['KLD']\n",
    "                    train_batches += 1\n",
    "\n",
    "                total_train_loss /= train_batches\n",
    "                r_train_loss /= train_batches\n",
    "                kl_train_loss /= train_batches\n",
    "\n",
    "                total_loss = 0\n",
    "                r_loss = 0\n",
    "                kl_loss = 0\n",
    "                batches = 0\n",
    "                for batch_X, batch_Y in val:\n",
    "                    batch_X = batch_X.float().cuda()\n",
    "                    batch_Y = batch_Y.float().cuda()\n",
    "                    recons, x, mu, log_var = model.forward(batch_X)\n",
    "                    loss = model.loss_function(recons, batch_Y, mu, log_var)\n",
    "                    total_loss += loss['loss']\n",
    "                    r_loss += loss['Reconstruction_Loss']\n",
    "                    kl_loss += loss['KLD']\n",
    "                    batches += 1\n",
    "\n",
    "                total_loss /= batches\n",
    "                r_loss /= batches\n",
    "                kl_loss /= batches\n",
    "            print(\"Train Loss\", total_train_loss)\n",
    "            print(\"Val Loss\", total_loss)\n",
    "            \n",
    "def train_model_old(model, train, val, n_epochs = 100):\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Start Epoch {epoch}\")\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loading_time = 0\n",
    "        for batch_X, batch_Y in train:\n",
    "            batch_X = batch_X.float().cuda()\n",
    "            recons, x, mu, log_var = model.forward(batch_X)\n",
    "            train_loss = model.loss_function(recons, batch_X, mu, log_var)['loss']\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_train_loss = 0\n",
    "            r_train_loss = 0\n",
    "            kl_train_loss = 0\n",
    "            train_batches = 0\n",
    "            for batch_X, batch_Y in train:\n",
    "                batch_X = batch_X.float().cuda()\n",
    "                recons, x, mu, log_var = model.forward(batch_X)\n",
    "                loss = model.loss_function(recons, batch_X, mu, log_var)\n",
    "                total_train_loss += loss['loss']\n",
    "                r_train_loss += loss['Reconstruction_Loss']\n",
    "                kl_train_loss += loss['KLD']\n",
    "                train_batches += 1\n",
    "\n",
    "            total_train_loss /= train_batches\n",
    "            r_train_loss /= train_batches\n",
    "            kl_train_loss /= train_batches\n",
    "\n",
    "            total_loss = 0\n",
    "            r_loss = 0\n",
    "            kl_loss = 0\n",
    "            batches = 0\n",
    "            for batch_X, batch_Y in val:\n",
    "                batch_X = batch_X.float().cuda()\n",
    "                recons, x, mu, log_var = model.forward(batch_X)\n",
    "                loss = model.loss_function(recons, batch_X, mu, log_var)\n",
    "                total_loss += loss['loss']\n",
    "                r_loss += loss['Reconstruction_Loss']\n",
    "                kl_loss += loss['KLD']\n",
    "                batches += 1\n",
    "\n",
    "            total_loss /= batches\n",
    "            r_loss /= batches\n",
    "            kl_loss /= batches\n",
    "        '''\n",
    "def top_k_recall(X_in, X_out, X_target, k = 20, mask_in = True):\n",
    "    if mask_in:\n",
    "        mask = X_in == False\n",
    "        X_out = X_out * mask\n",
    "    topk = torch.topk(X_out, k)\n",
    "    n = 0\n",
    "    total_recall = 0\n",
    "    for i in range(len(X_in)):\n",
    "        if int(X_target[i].sum()) == 0:\n",
    "            continue\n",
    "        selected = topk.indices[i]\n",
    "        total_recall += X_target[i][selected].sum() / X_target[i].sum()\n",
    "        n += 1\n",
    "    \n",
    "    return total_recall / n\n",
    "\n",
    "def n_recall(X_in, X_out, X_target, mask_in = True):\n",
    "    if mask_in:\n",
    "        mask = X_in == False\n",
    "        X_out = X_out * mask\n",
    "        X_target = X_target * mask\n",
    "    topk = torch.topk(X_out, int(X_target.sum(axis=1).max()), sorted=True)\n",
    "    n = 0\n",
    "    total_recall = 0\n",
    "    for i in range(len(X_in)):\n",
    "        if int(X_target[i].sum()) == 0:\n",
    "            continue\n",
    "        selected = topk.indices[i]\n",
    "        total_recall += X_target[i][selected[:int(X_target[i].sum())]].sum() / int(X_target[i].sum())\n",
    "        n += 1\n",
    "    #print(total_recall,n)\n",
    "    #print(len(X_in))\n",
    "    return total_recall / len(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import scipy\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pysrc.constants import datapath#, N_ITEMS, N_USERS\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_jaccard_sparse(csr):\n",
    "    \"\"\"Computes the Jaccard distance between the rows of `csr`,\n",
    "    smaller than the cut-off distance `epsilon`.\n",
    "    \"\"\"\n",
    "    csr = csr_matrix(csr).astype(bool).astype(int)\n",
    "\n",
    "    csr_rownnz = csr.getnnz(axis=1)\n",
    "    intrsct = csr.dot(csr.T)\n",
    "\n",
    "    nnz_i = np.repeat(csr_rownnz, intrsct.getnnz(axis=1))\n",
    "    unions = nnz_i + csr_rownnz[intrsct.indices] - intrsct.data\n",
    "    dists = intrsct.data / unions\n",
    "\n",
    "    out = csr_matrix((dists, intrsct.indices, intrsct.indptr), intrsct.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_sparse_uniform(sparse_X, p_relative = .1):\n",
    "    X = []\n",
    "    for i in range(sparse_train_data.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        X.append(torch.zeros(item_dim).bool())\n",
    "        row = sparse_X[i].coalesce().indices()[0]\n",
    "        X[-1][row] = 1\n",
    "    X = torch.stack(X)\n",
    "    return X\n",
    "    \n",
    "def sparse_to_ohe(data):\n",
    "    return sparsify_sparse_uniform(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def log_likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(torch.log(yhat+1e-6) * y, axis = 1))\n",
    "\n",
    "def likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(yhat * y, axis = 1))\n",
    "\n",
    "class GraphVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims = None,\n",
    "                 kl_weight = .2,\n",
    "                 sim = None\n",
    "                 ):\n",
    "        super(GraphVAE, self).__init__()\n",
    "        \n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.embeddings = torch.zeros(N_USERS, self.latent_dim).cuda()\n",
    "        \n",
    "        coo = sim.tocoo()\n",
    "        \n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "\n",
    "        self.neighbors = torch.sparse.FloatTensor(i, v, torch.Size(shape)).cuda()\n",
    "\n",
    "        self.neighbors_norms = torch.zeros(self.neighbors.shape[0])\n",
    "        \n",
    "        for i in range(self.neighbors.shape[0]):\n",
    "            self.neighbors_norms[i] = torch.sum(self.neighbors[i].to_dense())\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [512, 128]\n",
    "            \n",
    "        self.hidden_dims = hidden_dims\n",
    "            \n",
    "        modules.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        modules.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        modules.append(nn.LeakyReLU())\n",
    "        \n",
    "        # Build Encoder\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(2 * latent_dim, hidden_dims[-1])\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(nn.Linear(hidden_dims[-1], input_dim),\n",
    "                                         nn.Sigmoid()\n",
    "                                        )\n",
    "        \n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, self.hidden_dims[0])\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        result = F.normalize(result, p=1)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x: Tensor, uids):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        self.embeddings[uids] = z.detach()\n",
    "        neighbors = torch.stack([self.neighbors[uid].to_dense() for uid in uids])\n",
    "        \n",
    "        neighbor_embeds = ((neighbors @ self.embeddings).T / (self.neighbors_norms[uids] + 1e-5)).T\n",
    "        \n",
    "        z = torch.cat([z, neighbor_embeds], axis=1)\n",
    "        \n",
    "        return  [F.normalize(self.decode(z), 1, dim=1), x, mu, log_var]\n",
    "    \n",
    "        \n",
    "    def set_embeddings(self, x, uids):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        self.embeddings[uids] = z.detach()\n",
    "\n",
    "    def loss_function(self, recons, x, mu, log_var) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        kld_weight = self.kl_weight\n",
    "        recons_loss = log_likelihood_loss(x, recons)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_model(model, train, val, n_epochs = 100):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loading_time = 0\n",
    "        for batch_X, batch_Y, batch_ind in train:\n",
    "            batch_X = batch_X.float()\n",
    "            batch_Y = batch_Y.float()\n",
    "            recons, x, mu, log_var = model.forward(batch_X, batch_ind)\n",
    "            train_loss = model.loss_function(recons, batch_Y, mu, log_var)['loss']\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(time.time() - start_time)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Done Epoch {epoch}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                total_train_loss = 0\n",
    "                r_train_loss = 0\n",
    "                kl_train_loss = 0\n",
    "                train_batches = 0\n",
    "                for batch_X, batch_Y, batch_ind in train:\n",
    "                    batch_X = batch_X.float()\n",
    "                    batch_Y = batch_Y.float()\n",
    "                    recons, x, mu, log_var = model.forward(batch_X, batch_ind)\n",
    "                    loss = model.loss_function(recons, batch_Y, mu, log_var)\n",
    "                    total_train_loss += loss['loss']\n",
    "                    r_train_loss += loss['Reconstruction_Loss']\n",
    "                    kl_train_loss += loss['KLD']\n",
    "                    train_batches += 1\n",
    "\n",
    "                total_train_loss /= train_batches\n",
    "                r_train_loss /= train_batches\n",
    "                kl_train_loss /= train_batches\n",
    "\n",
    "                total_loss = 0\n",
    "                r_loss = 0\n",
    "                kl_loss = 0\n",
    "                batches = 0\n",
    "                for batch_X, batch_Y, batch_ind in val:\n",
    "                    batch_X = batch_X.float().cuda()\n",
    "                    batch_Y = batch_Y.float().cuda()\n",
    "                    recons, x, mu, log_var = model.forward(batch_X, batch_ind)\n",
    "                    loss = model.loss_function(recons, batch_Y, mu, log_var)\n",
    "                    total_loss += loss['loss']\n",
    "                    r_loss += loss['Reconstruction_Loss']\n",
    "                    kl_loss += loss['KLD']\n",
    "                    batches += 1\n",
    "\n",
    "                total_loss /= batches\n",
    "                r_loss /= batches\n",
    "                kl_loss /= batches\n",
    "                \n",
    "            print(\"Train Loss\", total_train_loss)\n",
    "            print(\"Val Loss\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_base(train_in_tensor, train_out_tensor, val_in_tensor, val_out_tensor, test_in_tensor, test_out_tensor):\n",
    "    train_dataset = TensorDataset(train_in_tensor.cuda(), (train_in_tensor + train_out_tensor).cuda(), train_uids.cuda())\n",
    "    val_dataset = TensorDataset(val_in_tensor, val_in_tensor + val_out_tensor)\n",
    "    test_dataset = TensorDataset(test_in_tensor, test_in_tensor + test_out_tensor)\n",
    "    # Create a data loader from the dataset\n",
    "    # Type of sampling and batch size are specified at this step\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    lr = 1e-3\n",
    "    model = VanillaVAE(input_dim = item_dim, latent_dim = 256, hidden_dims = [512, 256], kl_weight = 1).cuda()\n",
    "    \n",
    "    train_model(model, train_dataloader, val_dataloader, n_epochs = 30)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_n_recall_train = 0\n",
    "        total_n_train = 0\n",
    "        for batch_X, batch_Y in train_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda())[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_X, False)\n",
    "            total_n_recall_train += n_recall_batch * len(batch_X)\n",
    "            total_n_train += len(batch_X)\n",
    "        print(\"Train recons recall:\", total_n_recall_train / total_n_train)\n",
    "        train_recons_recall = total_n_recall_train / total_n_train\n",
    "        \n",
    "        total_n_recall_out = 0\n",
    "        total_n_out = 0\n",
    "        for batch_X, batch_Y in train_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda())[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_Y, True)\n",
    "            total_n_recall_out += n_recall_batch * len(batch_X)\n",
    "            total_n_out += len(batch_X)\n",
    "        print(\"Train pred recall:\", total_n_recall_out / total_n_out)\n",
    "        train_out_recall = total_n_recall_out / total_n_out\n",
    "        \n",
    "        total_n_recall_recons = 0\n",
    "        total_n_recons = 0\n",
    "        for batch_X, batch_Y in test_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda())[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_X, False)\n",
    "            total_n_recall_recons += n_recall_batch * len(batch_X)\n",
    "            total_n_recons += len(batch_X)\n",
    "        print(\"Test recons recall:\", total_n_recall_recons / total_n_recons)\n",
    "        test_recons_recall = total_n_recall_recons / total_n_recons\n",
    "        \n",
    "        total_n_recall_out = 0\n",
    "        total_n_out = 0\n",
    "        for batch_X, batch_Y in test_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda())[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_Y, True)\n",
    "            total_n_recall_out += n_recall_batch * len(batch_X)\n",
    "            total_n_out += len(batch_X)\n",
    "        print(\"Test outsample recall:\", total_n_recall_out / total_n_out)\n",
    "        test_outsample_recall = total_n_recall_out / total_n_out\n",
    "    return model, {\"train_recons\": train_recons_recall, \"train_out\": train_out_recall, \"test_recons\": test_recons_recall, \"test_out\": test_outsample_recall}\n",
    "\n",
    "def train_and_eval_graph(sim, train_in_tensor, train_out_tensor, train_uids, val_in_tensor, val_out_tensor, val_uids, test_in_tensor, test_out_tensor, test_uids):\n",
    "    train_dataset = TensorDataset(train_in_tensor.cuda(), (train_in_tensor + train_out_tensor).cuda(), train_uids.cuda())\n",
    "    val_dataset = TensorDataset(val_in_tensor, val_in_tensor + val_out_tensor, val_uids)\n",
    "    test_dataset = TensorDataset(test_in_tensor, test_in_tensor + test_out_tensor, test_uids)\n",
    "    # Create a data loader from the dataset\n",
    "    # Type of sampling and batch size are specified at this step\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "    lr = 2e-3\n",
    "    model = GraphVAE(input_dim = item_dim, latent_dim = 256, hidden_dims = [512, 256], kl_weight = 1, sim = sim).cuda()\n",
    "    \n",
    "    train_graph_model(model, train_dataloader, val_dataloader, n_epochs = 30)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y, batch_ind in train_dataloader:\n",
    "            model.set_embeddings(batch_X.float().cuda(), batch_ind)\n",
    "        for batch_X, batch_Y, batch_ind in val_dataloader:\n",
    "            model.set_embeddings(batch_X.float().cuda(), batch_ind)\n",
    "        for batch_X, batch_Y, batch_ind in test_dataloader:\n",
    "            model.set_embeddings(batch_X.float().cuda(), batch_ind)\n",
    "\n",
    "        total_n_recall_recons = 0\n",
    "        total_n_recons = 0\n",
    "        for batch_X, batch_Y, batch_ind in train_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda(), batch_ind)[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_X, False)\n",
    "            total_n_recall_recons += n_recall_batch * len(batch_ind)\n",
    "            total_n_recons += len(batch_ind)\n",
    "        print(\"Train recons recall:\", total_n_recall_recons / total_n_recons)\n",
    "        test_recons_recall = total_n_recall_recons / total_n_recons\n",
    "\n",
    "\n",
    "        total_n_recall_out = 0\n",
    "        total_n_out = 0\n",
    "        for batch_X, batch_Y, batch_ind in train_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda(), batch_ind)[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_Y, True)\n",
    "            total_n_recall_out += n_recall_batch * len(batch_X)\n",
    "            total_n_out += len(batch_X)\n",
    "        print(\"Train outsample recall:\", total_n_recall_out / total_n_out)\n",
    "        test_outsample_recall = total_n_recall_out / total_n_out\n",
    "\n",
    "        total_n_recall_recons = 0\n",
    "        total_n_recons = 0\n",
    "        for batch_X, batch_Y, batch_ind in test_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda(), batch_ind)[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_X, False)\n",
    "            total_n_recall_recons += n_recall_batch * len(batch_X)\n",
    "            total_n_recons += len(batch_X)\n",
    "        print(\"Test recons recall:\", total_n_recall_recons / total_n_recons)\n",
    "        test_recons_recall = total_n_recall_recons / total_n_recons\n",
    "\n",
    "\n",
    "        total_n_recall_out = 0\n",
    "        total_n_out = 0\n",
    "        for batch_X, batch_Y, batch_ind in test_dataloader:\n",
    "            batch_out = model.forward(batch_X.float().cuda(), batch_ind)[0].detach().cpu()\n",
    "            n_recall_batch = n_recall(batch_X, batch_out, batch_Y, True)\n",
    "            total_n_recall_out += n_recall_batch * len(batch_ind)\n",
    "            total_n_out += len(batch_ind)\n",
    "        print(\"Test outsample recall:\", total_n_recall_out / total_n_out)\n",
    "        test_outsample_recall = total_n_recall_out / total_n_out\n",
    "    return model, {\"train_recons\": train_recons_recall, \"train_out\": train_out_recall, \"test_recons\": test_recons_recall, \"test_out\": test_outsample_recall}\n",
    "\n",
    "def compare_on_data(train_path, test_path):\n",
    "    sparse_train_data = torch.load(train_path)\n",
    "    sparse_test_data = pickle.load(open(test_path, \"rb\"))\n",
    "    sim = pairwise_jaccard_sparse(sparse_train_data.to_dense())\n",
    "    for i in range(sim.shape[0]):\n",
    "        sim[i,i] = 0\n",
    "    dev_df = sparse_train_data.bool().to_dense()\n",
    "    test_df = list_batch_to_ohe(pd.Series(sparse_test_data))\n",
    "    test_mask = torch.zeros(dev_df.shape[0])\n",
    "    test_mask[test_ids[test_ids < dev_df.shape[0]]] = 1\n",
    "    \n",
    "    dev_in = dev_df[test_mask == 0]\n",
    "    dev_out = test_df[test_mask == 0]\n",
    "    test_in_tensor = dev_df[test_mask == 1]\n",
    "    test_out_tensor = test_df[test_mask == 1]\n",
    "\n",
    "    train_uids = (1-test_mask).nonzero().reshape(-1)[:int(.7*len(dev_in))]\n",
    "    val_uids = (1-test_mask).nonzero().reshape(-1)[int(.7*len(dev_in)):]\n",
    "    test_uids = test_mask.nonzero().reshape(-1)\n",
    "\n",
    "    train_in_tensor = dev_in[:int(.7*len(dev_in))]\n",
    "    val_in_tensor = dev_in[int(.7*len(dev_in)):]\n",
    "\n",
    "    train_out_tensor = dev_out[:int(.7*len(dev_in))]\n",
    "    val_out_tensor = dev_out[int(.7*len(dev_in)):]\n",
    "\n",
    "    graph_model, graph_results = train_and_eval_graph(sim, train_in_tensor, train_out_tensor, train_uids, val_in_tensor, val_out_tensor, val_uids, test_in_tensor, test_out_tensor, test_uids)\n",
    "    \n",
    "    base_model, base_results = train_and_eval_base(train_in_tensor, train_out_tensor, val_in_tensor, val_out_tensor, test_in_tensor, test_out_tensor)\n",
    "    \n",
    "    return base_model, base_results, graph_model, graph_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_on_data(datapath(f\"train/uniform/uniform100_data.pt\"), datapath(f\"test/uniform/uniform100_test_indices.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_on_data(datapath(f\"full_data.pt\"), datapath(f\"full_data_test_indices.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_data = torch.load(datapath(f\"full_data.pt\"))\n",
    "sparse_test_data = pickle.load(open(datapath(f\"full_data_test_indices.pickle\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = pairwise_jaccard_sparse(sparse_train_data.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo = sim.tocoo()\n",
    "        \n",
    "values = coo.data\n",
    "indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = coo.shape\n",
    "\n",
    "neighbors = torch.sparse.FloatTensor(i, v, torch.Size(shape)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::sum.dim_IntList' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sum.dim_IntList' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:30798 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:43635 [kernel]\nHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterMeta.cpp:26815 [kernel]\nPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nFPGA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nORT: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nVulkan: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMetal: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nCustomRNGKeyId: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMkldnnCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nSparseCsrCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nSparseCsrCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nNestedTensorCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterNestedTensorCPU.cpp:457 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16903 [kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:328 [kernel]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\BatchRulesReduceOps.cpp:371 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\BatchingRegistrations.cpp:1068 [kernel]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6932\\1751679136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::sum.dim_IntList' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sum.dim_IntList' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:30798 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:43635 [kernel]\nHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterMeta.cpp:26815 [kernel]\nPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nFPGA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nORT: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nVulkan: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMetal: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nQuantizedPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nCustomRNGKeyId: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nMkldnnCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nSparseCsrCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nSparseCsrCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCompositeExplicitAutogradNonFunctional.cpp:21389 [default backend kernel]\nNestedTensorCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterNestedTensorCPU.cpp:457 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:16903 [kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:328 [kernel]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\BatchRulesReduceOps.cpp:371 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\BatchingRegistrations.cpp:1068 [kernel]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "torch.sum(neighbors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0686, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(neighbors[0].coalesce().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
