{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pysrc.constants import datapath, N_ITEMS, N_USERS\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indices = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset (No Sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[    0,     0,     0,  ..., 52642, 52642, 52642],\n",
      "                       [    0,     1,     2,  ..., 23186, 10690, 10874]]),\n",
      "       values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
      "       size=(52643, 91599), nnz=2380730, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "user_ids = []\n",
    "item_ids = []\n",
    "values = []\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        user_id, items = line.split(maxsplit=1)\n",
    "        items = [int(id) for id in items.split()]\n",
    "        user_ids += [int(user_id)] * len(items)\n",
    "        values += [1] * len(items)\n",
    "        item_ids += items\n",
    "\n",
    "train_data = torch.sparse_coo_tensor([user_ids, item_ids], values, (N_USERS, N_ITEMS))\n",
    "torch.save(train_data, datapath(\"full_data.pt\"))\n",
    "print(train_data)\n",
    "\n",
    "dataset_indices[\"full_data\"] = list(range(N_USERS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 0)\n",
    "\n",
    "for sparsity in [0, .01, .02, .03, .05, .1, .2, .3, .4, .5, .6, .7, .9]:\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    with open(datapath(\"train.txt\")) as file:\n",
    "        for line in file:\n",
    "            user_id, items = line.split(maxsplit=1)\n",
    "            items = [int(id) for id in items.split()]\n",
    "            user_ids += [int(user_id)] * len(items)\n",
    "            values += [1] * len(items)\n",
    "            item_ids += items\n",
    "\n",
    "    indices = np.random.choice(range(len(user_ids)), \n",
    "                               size = int(len(user_ids) * (1 - sparsity)), \n",
    "                               replace = False)\n",
    "    user_ids = np.array(user_ids)[indices]\n",
    "    item_ids = np.array(item_ids)[indices]\n",
    "    values = np.array(values)[indices]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(np.array([user_ids, item_ids]), values, (N_USERS, N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"uniform/uniform{int(100*(1 - sparsity))}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"uniform{int(100*(1 - sparsity))}_data\"] = list(range(N_USERS))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top (User) Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "train_dict = {}\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "\n",
    "user_sparsities = {int(k):len(v)/N_ITEMS for k,v in train_dict.items()}\n",
    "qs = np.array([list(user_sparsities.keys()), list(user_sparsities.values())])\n",
    "\n",
    "for q in [70, 75, 80, 85, 90, 95]:\n",
    "    quant = np.quantile(qs[1, :], q * 0.01)\n",
    "    indices = qs[0, qs[1, :] < quant]\n",
    "\n",
    "    train_is = train_full.coalesce().indices().numpy()\n",
    "    train_is = train_is[:, np.isin(train_is[0, :], indices)]\n",
    "\n",
    "    reindex = 0\n",
    "    current_value = train_is[0, 0]\n",
    "    for i in range(train_is.shape[1]):\n",
    "        if (train_is[0, i] == current_value):\n",
    "            train_is[0, i] = reindex\n",
    "        else:\n",
    "            current_value = train_is[0, i]\n",
    "            reindex += 1\n",
    "            train_is[0, i] = reindex\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (len(indices), N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"top/top{q}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"top{q}_data\"] = indices    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "train_dict = {}\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "\n",
    "user_sparsities = {int(k):len(v)/N_ITEMS for k,v in train_dict.items()}\n",
    "qs = np.array([list(user_sparsities.keys()), list(user_sparsities.values())])\n",
    "\n",
    "for radius in [5, 10, 15, 20, 25, 30]:\n",
    "    low = np.quantile(qs[1, :], 0.5 - radius * 0.01)\n",
    "    high = np.quantile(qs[1, :], 0.5 + radius * 0.01)\n",
    "    indices = qs[0, (qs[1, :] < low) | (qs[1, :] > high)]\n",
    "\n",
    "    train_is = train_full.coalesce().indices().numpy()\n",
    "    train_is = train_is[:, np.isin(train_is[0, :], indices)]\n",
    "\n",
    "    reindex = 0\n",
    "    current_value = train_is[0, 0]\n",
    "    for i in range(train_is.shape[1]):\n",
    "        if (train_is[0, i] == current_value):\n",
    "            train_is[0, i] = reindex\n",
    "        else:\n",
    "            current_value = train_is[0, i]\n",
    "            reindex += 1\n",
    "            train_is[0, i] = reindex\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (len(indices), N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"middle/middle{radius}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"middle{radius}_data\"] = indices    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGR Degredation\n",
    "\n",
    "Reference: https://www.sciencedirect.com/science/article/pii/S0957417410010985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard matrix between users and determine connected components of user graph\n",
    "\n",
    "def pairwise_jaccard_sparse(csr, epsilon):\n",
    "    \"\"\"\n",
    "    Reference: https://stackoverflow.com/questions/32805916/compute-jaccard-distances-on-sparse-matrix\n",
    "    Computes the Jaccard distance between the rows of `csr`,\n",
    "    smaller than the cut-off distance `epsilon`.\n",
    "    \"\"\"\n",
    "    assert(0 < epsilon < 1)\n",
    "    csr = csr_matrix(csr).astype(bool).astype(int)\n",
    "\n",
    "    csr_rownnz = csr.getnnz(axis=1)\n",
    "    intrsct = csr.dot(csr.T)\n",
    "\n",
    "    nnz_i = np.repeat(csr_rownnz, intrsct.getnnz(axis=1))\n",
    "    unions = nnz_i + csr_rownnz[intrsct.indices] - intrsct.data\n",
    "    dists = 1.0 - intrsct.data / unions\n",
    "\n",
    "    mask = (dists > 0) & (dists <= epsilon)\n",
    "    data = dists[mask]\n",
    "    indices = intrsct.indices[mask]\n",
    "\n",
    "    rownnz = np.add.reduceat(mask, intrsct.indptr[:-1])\n",
    "    indptr = np.r_[0, np.cumsum(rownnz)]\n",
    "\n",
    "    out = csr_matrix((data, indices, indptr), intrsct.shape)\n",
    "    return out\n",
    "\n",
    "train_full = torch.load(datapath(\"full_data.pt\")).coalesce()\n",
    "inds = train_full.indices().numpy()\n",
    "\n",
    "X = csr_matrix((np.ones(inds.shape[1]), (inds[0, :], inds[1, :])), shape = (N_USERS, N_ITEMS))\n",
    "similarities = pairwise_jaccard_sparse(X, 0.87)\n",
    "\n",
    "N, components = scipy.sparse.csgraph.connected_components(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
