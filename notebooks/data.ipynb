{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import scipy\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pysrc.constants import datapath, N_ITEMS, N_USERS\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indices = {}\n",
    "for dir_name in [\"uniform\", \"top\", \"middle\"]:\n",
    "    Path(datapath(dir_name)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset (No Sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[    0,     0,     0,  ..., 52642, 52642, 52642],\n",
      "                       [    0,     1,     2,  ..., 23186, 10690, 10874]]),\n",
      "       values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
      "       size=(52643, 91599), nnz=2380730, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "user_ids = []\n",
    "item_ids = []\n",
    "values = []\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        user_id, items = line.split(maxsplit=1)\n",
    "        items = [int(id) for id in items.split()]\n",
    "        user_ids += [int(user_id)] * len(items)\n",
    "        values += [1] * len(items)\n",
    "        item_ids += items\n",
    "\n",
    "train_data = torch.sparse_coo_tensor([user_ids, item_ids], values, (N_USERS, N_ITEMS))\n",
    "torch.save(train_data, datapath(\"full_data.pt\"))\n",
    "print(train_data)\n",
    "\n",
    "dataset_indices[\"full_data\"] = list(range(N_USERS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 0)\n",
    "\n",
    "for sparsity in [0, .01, .02, .03, .05, .1, .2, .3, .4, .5, .6, .7, .9]:\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    with open(datapath(\"train.txt\")) as file:\n",
    "        for line in file:\n",
    "            user_id, items = line.split(maxsplit=1)\n",
    "            items = [int(id) for id in items.split()]\n",
    "            user_ids += [int(user_id)] * len(items)\n",
    "            values += [1] * len(items)\n",
    "            item_ids += items\n",
    "\n",
    "    indices = np.random.choice(range(len(user_ids)), \n",
    "                               size = int(len(user_ids) * (1 - sparsity)), \n",
    "                               replace = False)\n",
    "    user_ids = np.array(user_ids)[indices]\n",
    "    item_ids = np.array(item_ids)[indices]\n",
    "    values = np.array(values)[indices]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(np.array([user_ids, item_ids]), values, (N_USERS, N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"uniform/uniform{int(100*(1 - sparsity))}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"uniform{int(100*(1 - sparsity))}_data\"] = list(range(N_USERS))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top (Item) Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictonary of users who have rated each item (i.e. invert train_dict)\n",
    "train_dict = {}\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "train_dict = {int(k):[int(id) for id in v] for k,v in train_dict.items()}\n",
    "\n",
    "item_users_dict = defaultdict(lambda: set())\n",
    "for k,v in train_dict.items():\n",
    "    for x in v:\n",
    "        item_users_dict[x].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "item_sparsities = {int(k):len(v)/N_USERS for k,v in item_users_dict.items()}\n",
    "qs = np.array([list(item_sparsities.keys()), list(item_sparsities.values())])\n",
    "\n",
    "for q in [70, 75, 80, 85, 90, 95]:\n",
    "    quant = np.quantile(qs[1, :], q * 0.01)\n",
    "    indices = qs[0, qs[1, :] < quant]\n",
    "\n",
    "    train_is = train_full.coalesce().indices().numpy()\n",
    "    train_is = train_is[:, np.isin(train_is[1, :], indices)]\n",
    "\n",
    "    order = np.argsort(train_is[1, :])\n",
    "    train_is = train_is[:, order]\n",
    "\n",
    "    reindex = 0\n",
    "    current_value = train_is[1, 0]\n",
    "    for i in range(train_is.shape[1]):\n",
    "        if (train_is[1, i] == current_value):\n",
    "            train_is[1, i] = reindex\n",
    "        else:\n",
    "            current_value = train_is[1, i]\n",
    "            reindex += 1\n",
    "            train_is[1, i] = reindex\n",
    "\n",
    "    order = np.argsort(train_is[0, :])\n",
    "    train_is = train_is[:, order]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (N_USERS, len(indices)))\n",
    "    torch.save(train_data, datapath(f\"top/top{q}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"top{q}_data\"] = indices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER PRUNING (NOT USED)\n",
    "# train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "# train_dict = {}\n",
    "# with open(datapath(\"train.txt\")) as file:\n",
    "#     for line in file:\n",
    "#         (user_id, items) = line.split(maxsplit=1)\n",
    "#         train_dict[user_id] = items.split()\n",
    "\n",
    "# user_sparsities = {int(k):len(v)/N_ITEMS for k,v in train_dict.items()}\n",
    "# qs = np.array([list(user_sparsities.keys()), list(user_sparsities.values())])\n",
    "\n",
    "# for q in [70, 75, 80, 85, 90, 95]:\n",
    "#     quant = np.quantile(qs[1, :], q * 0.01)\n",
    "#     indices = qs[0, qs[1, :] < quant]\n",
    "\n",
    "#     train_is = train_full.coalesce().indices().numpy()\n",
    "#     train_is = train_is[:, np.isin(train_is[0, :], indices)]\n",
    "\n",
    "#     reindex = 0\n",
    "#     current_value = train_is[0, 0]\n",
    "#     for i in range(train_is.shape[1]):\n",
    "#         if (train_is[0, i] == current_value):\n",
    "#             train_is[0, i] = reindex\n",
    "#         else:\n",
    "#             current_value = train_is[0, i]\n",
    "#             reindex += 1\n",
    "#             train_is[0, i] = reindex\n",
    "\n",
    "#     train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (len(indices), N_ITEMS))\n",
    "#     torch.save(train_data, datapath(f\"top/top{q}_data.pt\"))\n",
    "\n",
    "#     dataset_indices[f\"top{q}_data\"] = indices    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictonary of users who have rated each item (i.e. invert train_dict)\n",
    "train_dict = {}\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "train_dict = {int(k):[int(id) for id in v] for k,v in train_dict.items()}\n",
    "\n",
    "item_users_dict = defaultdict(lambda: set())\n",
    "for k,v in train_dict.items():\n",
    "    for x in v:\n",
    "        item_users_dict[x].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "item_sparsities = {int(k):len(v)/N_USERS for k,v in item_users_dict.items()}\n",
    "qs = np.array([list(item_sparsities.keys()), list(item_sparsities.values())])\n",
    "\n",
    "for radius in [5, 10, 15, 20, 25, 30]:\n",
    "    low = np.quantile(qs[1, :], 0.5 - radius * 0.01)\n",
    "    high = np.quantile(qs[1, :], 0.5 + radius * 0.01)\n",
    "    indices = qs[0, (qs[1, :] < low) | (qs[1, :] > high)]\n",
    "\n",
    "    train_is = train_full.coalesce().indices().numpy()\n",
    "    train_is = train_is[:, np.isin(train_is[1, :], indices)]\n",
    "\n",
    "    order = np.argsort(train_is[1, :])\n",
    "    train_is = train_is[:, order]\n",
    "\n",
    "    reindex = 0\n",
    "    current_value = train_is[1, 0]\n",
    "    for i in range(train_is.shape[1]):\n",
    "        if (train_is[1, i] == current_value):\n",
    "            train_is[1, i] = reindex\n",
    "        else:\n",
    "            current_value = train_is[1, i]\n",
    "            reindex += 1\n",
    "            train_is[1, i] = reindex\n",
    "\n",
    "    order = np.argsort(train_is[0, :])\n",
    "    train_is = train_is[:, order]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (N_USERS, len(indices)))\n",
    "    torch.save(train_data, datapath(f\"middle/middle{radius}_data.pt\"))\n",
    "\n",
    "    dataset_indices[f\"middle{radius}_data\"] = indices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER SPARSITIES (NOT USED)\n",
    "# train_full = torch.load(datapath(\"full_data.pt\"))\n",
    "\n",
    "# train_dict = {}\n",
    "# with open(datapath(\"train.txt\")) as file:\n",
    "#     for line in file:\n",
    "#         (user_id, items) = line.split(maxsplit=1)\n",
    "#         train_dict[user_id] = items.split()\n",
    "\n",
    "# user_sparsities = {int(k):len(v)/N_ITEMS for k,v in train_dict.items()}\n",
    "# qs = np.array([list(user_sparsities.keys()), list(user_sparsities.values())])\n",
    "\n",
    "# for radius in [5, 10, 15, 20, 25, 30]:\n",
    "#     low = np.quantile(qs[1, :], 0.5 - radius * 0.01)\n",
    "#     high = np.quantile(qs[1, :], 0.5 + radius * 0.01)\n",
    "#     indices = qs[0, (qs[1, :] < low) | (qs[1, :] > high)]\n",
    "\n",
    "#     train_is = train_full.coalesce().indices().numpy()\n",
    "#     train_is = train_is[:, np.isin(train_is[0, :], indices)]\n",
    "\n",
    "#     reindex = 0\n",
    "#     current_value = train_is[0, 0]\n",
    "#     for i in range(train_is.shape[1]):\n",
    "#         if (train_is[0, i] == current_value):\n",
    "#             train_is[0, i] = reindex\n",
    "#         else:\n",
    "#             current_value = train_is[0, i]\n",
    "#             reindex += 1\n",
    "#             train_is[0, i] = reindex\n",
    "\n",
    "#     train_data = torch.sparse_coo_tensor(train_is, np.ones(train_is.shape[1]), (len(indices), N_ITEMS))\n",
    "#     torch.save(train_data, datapath(f\"middle/middle{radius}_data.pt\"))\n",
    "\n",
    "#     dataset_indices[f\"middle{radius}_data\"] = indices    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGR and UIS1 Degredation\n",
    "\n",
    "Reference: https://www.sciencedirect.com/science/article/pii/S0957417410010985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard matrix between users and determine connected components of user graph\n",
    "\n",
    "def pairwise_jaccard_sparse(csr, epsilon):\n",
    "    \"\"\"\n",
    "    Reference: https://stackoverflow.com/questions/32805916/compute-jaccard-distances-on-sparse-matrix\n",
    "    Computes the Jaccard distance between the rows of `csr`, smaller than the cut-off distance `epsilon`.\n",
    "    \"\"\"\n",
    "    assert(0 < epsilon < 1)\n",
    "    csr = csr_matrix(csr).astype(bool).astype(int)\n",
    "\n",
    "    csr_rownnz = csr.getnnz(axis=1)\n",
    "    intrsct = csr.dot(csr.T)\n",
    "\n",
    "    nnz_i = np.repeat(csr_rownnz, intrsct.getnnz(axis=1))\n",
    "    unions = nnz_i + csr_rownnz[intrsct.indices] - intrsct.data\n",
    "    dists = 1.0 - intrsct.data / unions\n",
    "\n",
    "    mask = (dists > 0) & (dists <= epsilon)\n",
    "    data = dists[mask]\n",
    "    indices = intrsct.indices[mask]\n",
    "\n",
    "    rownnz = np.add.reduceat(mask, intrsct.indptr[:-1])\n",
    "    indptr = np.r_[0, np.cumsum(rownnz)]\n",
    "\n",
    "    out = csr_matrix((data, indices, indptr), intrsct.shape)\n",
    "    return out\n",
    "\n",
    "train_full = torch.load(datapath(\"full_data.pt\")).coalesce()\n",
    "inds = train_full.indices().numpy()\n",
    "\n",
    "X = csr_matrix((np.ones(inds.shape[1]), (inds[0, :], inds[1, :])), shape = (N_USERS, N_ITEMS))\n",
    "similarities = pairwise_jaccard_sparse(X, 0.87)\n",
    "\n",
    "N, components = scipy.sparse.csgraph.connected_components(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x52643 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coo_matrix(similarities)\n",
    "similarities.getrow(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictonary of users who have rated each item (i.e. invert train_dict)\n",
    "train_dict = {}\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "train_dict = {int(k):[int(id) for id in v] for k,v in train_dict.items()}\n",
    "\n",
    "item_users_dict = defaultdict(lambda: set())\n",
    "for k,v in train_dict.items():\n",
    "    for x in v:\n",
    "        item_users_dict[x].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 52642, 52642, 52642],\n",
       "       [    0,     1,     2, ..., 32608, 32776, 37931]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.indices().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 159, 914, 1052, 1749, 2713}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_indices = np.vstack([np.arange(len(components)), components])\n",
    "set(comp_indices[0, comp_indices[1, :] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_indices = np.vstack([np.arange(len(components)), components])\n",
    "\n",
    "def get_local_neighbors(user_id):\n",
    "    pass\n",
    "\n",
    "def get_global_neighbors(user_id):\n",
    "    comp_indices = np.vstack([np.arange(len(components)), components])\n",
    "    return set(comp_indices[0, comp_indices[1, :] == user_id])\n",
    "\n",
    "LGRs = []\n",
    "UIS1s = []\n",
    "for user_id in tqdm(range(N_USERS)):\n",
    "    local_neighbors = get_local_neighbors(user_id)\n",
    "    global_neighbors = get_global_neighbors(user_id)\n",
    "    for item in range(N_ITEMS):\n",
    "        # Can speed up, there are simple cases where L_ui = 0, so LGR = UIS1 = 1\n",
    "        L_ui = len(local_neighbors & item_users_dict[item])\n",
    "        G_ui = len(global_neighbors & item_users_dict[item])\n",
    "        N_i = len(item_users_dict[item])\n",
    "        LGRs.append(1 - L_ui / G_ui)\n",
    "        UIS1s.append(1 - L_ui / N_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath(\"dataset_indices.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(dataset_indices, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
