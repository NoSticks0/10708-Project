{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import scipy\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pysrc.constants import datapath, N_ITEMS, N_USERS, N_ITEMS_SUB, N_USERS_SUB\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_name in [\"uniform\", \"top\", \"middle\", \"LGR\", \"UIS1\"]:\n",
    "    Path(datapath(\"train/\" + dir_name)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(datapath(\"test/\" + dir_name)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset (No Sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[    0,     0,     0,  ...,  1000,  1000,  1000],\n",
      "                       [    0,     1,     2,  ...,  7856,  8922, 24260]]),\n",
      "       values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
      "       size=(1001, 91599), nnz=109986, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "user_ids = []\n",
    "item_ids = []\n",
    "values = []\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        user_id, items = line.split(maxsplit=1)\n",
    "        items = [int(id) for id in items.split()]\n",
    "        user_ids += [int(user_id)] * len(items)\n",
    "        values += [1] * len(items)\n",
    "        item_ids += items\n",
    "        i += 1\n",
    "        if i > 1000:\n",
    "            break\n",
    "\n",
    "train_data = torch.sparse_coo_tensor([user_ids, item_ids], values, (1001, N_ITEMS))\n",
    "torch.save(train_data, datapath(\"full_data.pt\"))\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            test_dict[user_id] = items.split()\n",
    "        else:\n",
    "            test_dict[split[0]] = ''\n",
    "        i += 1\n",
    "        if i > 1000:\n",
    "            break\n",
    "test_dict = {int(k):[int(id) for id in v] for k,v in test_dict.items()}\n",
    "with open(datapath(\"full_data_test_indices20.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 0)\n",
    "\n",
    "for sparsity in [0]:\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    with open(datapath(\"train.txt\")) as file:\n",
    "        for line in file:\n",
    "            user_id, items = line.split(maxsplit=1)\n",
    "            items = [int(id) for id in items.split()]\n",
    "            user_ids += [int(user_id)] * len(items)\n",
    "            values += [1] * len(items)\n",
    "            item_ids += items\n",
    "\n",
    "    indices = np.random.choice(range(len(user_ids)), \n",
    "                               size = int(len(user_ids) * (1 - sparsity)), \n",
    "                               replace = False)\n",
    "    user_ids = np.array(user_ids)[indices]\n",
    "    item_ids = np.array(item_ids)[indices]\n",
    "    values = np.array(values)[indices]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(np.array([user_ids, item_ids]), values, (N_USERS, N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"train/uniform/uniform{int(100*(1 - sparsity))}_data.pt\"))\n",
    "\n",
    "    test_dict = {}\n",
    "    with open(datapath(\"test.txt\")) as file:\n",
    "        for line in file:\n",
    "            split = line.split(maxsplit=1)\n",
    "            if (len(split) > 1):\n",
    "                (user_id, items) = split\n",
    "                test_dict[user_id] = items.split()\n",
    "            else:\n",
    "                test_dict[split[0]] = ''\n",
    "\n",
    "    test_dict = {int(k):[int(id) for id in v] for k,v in test_dict.items()}\n",
    "    with open(datapath(f\"test/uniform/uniform{int(100*(1 - sparsity))}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_21184\\3122899315.py\", line 18, in <module>\n",
      "    user_ids = np.array(user_ids)[indices]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21184\\3122899315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                replace = False)\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0muser_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mitem_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2098\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2099\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2100\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2100\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2102\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1368\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             )\n\u001b[0;32m   1270\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1125\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed = 0)\n",
    "\n",
    "for sparsity in [0, .01, .02, .03, .05, .1, .2, .3, .4, .5, .6, .7, .9]:\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    with open(datapath(\"train.txt\")) as file:\n",
    "        for line in file:\n",
    "            user_id, items = line.split(maxsplit=1)\n",
    "            items = [int(id) for id in items.split()]\n",
    "            user_ids += [int(user_id)] * len(items)\n",
    "            values += [1] * len(items)\n",
    "            item_ids += items\n",
    "\n",
    "    indices = np.random.choice(range(len(user_ids)), \n",
    "                               size = int(len(user_ids) * (1 - sparsity)), \n",
    "                               replace = False)\n",
    "    user_ids = np.array(user_ids)[indices]\n",
    "    item_ids = np.array(item_ids)[indices]\n",
    "    values = np.array(values)[indices]\n",
    "\n",
    "    train_data = torch.sparse_coo_tensor(np.array([user_ids, item_ids]), values, (N_USERS, N_ITEMS))\n",
    "    torch.save(train_data, datapath(f\"train/uniform/uniform{int(100*(1 - sparsity))}_data.pt\"))\n",
    "\n",
    "    test_dict = {}\n",
    "    with open(datapath(\"test.txt\")) as file:\n",
    "        for line in file:\n",
    "            split = line.split(maxsplit=1)\n",
    "            if (len(split) > 1):\n",
    "                (user_id, items) = split\n",
    "                test_dict[user_id] = items.split()\n",
    "            else:\n",
    "                test_dict[split[0]] = ''\n",
    "\n",
    "    test_dict = {int(k):[int(id) for id in v] for k,v in test_dict.items()}\n",
    "    with open(datapath(f\"test/uniform/uniform{int(100*(1 - sparsity))}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = []\n",
    "item_ids = []\n",
    "values = []\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            items = [int(id) for id in items.split()]\n",
    "            user_ids += [int(user_id)] * len(items)\n",
    "            values += [1] * len(items)\n",
    "            item_ids += items\n",
    "\n",
    "train_data = torch.load(datapath(\"full_data.pt\"))\n",
    "test_data = torch.sparse_coo_tensor([user_ids, item_ids], values, (N_USERS, N_ITEMS))\n",
    "all_data = train_data + test_data\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_coo(coo, cols):\n",
    "    coo_indices = coo.coalesce().indices().numpy()\n",
    "    selected_indices = coo_indices[:, np.isin(coo_indices[1, :], cols)]\n",
    "\n",
    "    coo_values = coo.coalesce().values().numpy()\n",
    "    selected_values = coo_values[np.isin(coo_indices[1, :], cols)]\n",
    "\n",
    "    cols = np.array(cols)\n",
    "    adj = {}\n",
    "    for i in range(coo.shape[1]):\n",
    "        adj[i] = i - np.sum(cols < i)\n",
    "\n",
    "    for i in range(selected_indices.shape[1]):\n",
    "        selected_indices[1, i] -= adj[selected_indices[1, i]]\n",
    "\n",
    "    return torch.sparse_coo_tensor(selected_indices, selected_values, (coo.shape[0], len(cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple unit test\n",
    "i = [[0, 1, 1, 2], [0, 1, 3, 0]]\n",
    "v = [1, 2, 1, 3]\n",
    "\n",
    "T = torch.sparse_coo_tensor(i, v, (3, 5))\n",
    "print(T.to_dense())\n",
    "print(select_columns_coo(T, [0, 2, 3, 4]).to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_user_dict(coo):\n",
    "    coo_indices = coo.coalesce().indices().numpy()\n",
    "    out_dict = {}\n",
    "    for i in range(N_USERS):\n",
    "        out_dict[i] = list(coo_indices[1, coo_indices[0, :] == i])\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple unit test\n",
    "i = [[0, 1, 1, 2], [0, 1, 3, 0]]\n",
    "v = [1, 2, 1, 3]\n",
    "\n",
    "T = torch.sparse_coo_tensor(i, v, (3, 5))\n",
    "print(T.to_dense())\n",
    "print(sparse_to_user_dict(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top (Item) Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = all_data.coalesce().indices().numpy()\n",
    "X = csr_matrix((np.ones(inds.shape[1]), (inds[0, :], inds[1, :])), shape = (N_USERS, N_ITEMS))\n",
    "totals = X.sum(axis = 0).A.flatten()\n",
    "\n",
    "item_sparsities = {}\n",
    "for i in range(len(totals)):\n",
    "    item_sparsities[i] = totals[i] / N_ITEMS\n",
    "qs = np.array([list(item_sparsities.keys()), list(item_sparsities.values())])\n",
    "\n",
    "for q in tqdm([70, 75, 80, 85, 90, 95]):\n",
    "    quant = np.quantile(qs[1, :], q * 0.01)\n",
    "    indices = qs[0, qs[1, :] >= quant]\n",
    "    \n",
    "    dense_train_data = train_data.bool().to_dense().clone()\n",
    "    dense_test_data = test_data.bool().to_dense().clone()\n",
    "    dense_train_data[:, indices] = 0\n",
    "    dense_test_data[:, indices] = 0\n",
    "    torch.save(dense_train_data.to_sparse(), datapath(f\"train/top/top{q}_data.pt\"))\n",
    "    \n",
    "    test_dict = sparse_to_user_dict(dense_test_data.to_sparse())\n",
    "\n",
    "    with open(datapath(f\"test/top/top{q}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = all_data.coalesce().indices().numpy()\n",
    "X = csr_matrix((np.ones(inds.shape[1]), (inds[0, :], inds[1, :])), shape = (N_USERS, N_ITEMS))\n",
    "totals = X.sum(axis = 0).A.flatten()\n",
    "\n",
    "item_sparsities = {}\n",
    "for i in range(len(totals)):\n",
    "    item_sparsities[i] = totals[i] / N_ITEMS\n",
    "qs = np.array([list(item_sparsities.keys()), list(item_sparsities.values())])\n",
    "\n",
    "for radius in tqdm([5, 10, 15, 20, 25, 30]):\n",
    "    low = np.quantile(qs[1, :], 0.5 - radius * 0.01)\n",
    "    high = np.quantile(qs[1, :], 0.5 + radius * 0.01)\n",
    "    indices = qs[0, (qs[1, :] < low) | (qs[1, :] > high)]\n",
    "\n",
    "    dense_train_data = train_data.bool().to_dense().clone()\n",
    "    dense_test_data = test_data.bool().to_dense().clone()\n",
    "    dense_train_data[:, indices] = 0\n",
    "    dense_test_data[:, indices] = 0\n",
    "    torch.save(dense_train_data.to_sparse(), datapath(f\"train/middle/middle{radius}_data.pt\"))\n",
    "    \n",
    "    test_dict = sparse_to_user_dict(dense_test_data.to_sparse())\n",
    "\n",
    "    with open(datapath(f\"test/middle/middle{radius}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGR and UIS1 Degredation\n",
    "\n",
    "Reference: https://www.sciencedirect.com/science/article/pii/S0957417410010985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard matrix between users and determine connected components of user graph\n",
    "\n",
    "def pairwise_jaccard_sparse(csr, epsilon):\n",
    "    \"\"\"\n",
    "    Reference: https://stackoverflow.com/questions/32805916/compute-jaccard-distances-on-sparse-matrix\n",
    "    Computes the Jaccard distance between the rows of `csr`, smaller than the cut-off distance `epsilon`.\n",
    "    \"\"\"\n",
    "    assert(0 < epsilon < 1)\n",
    "    csr = csr_matrix(csr).astype(bool).astype(int)\n",
    "\n",
    "    csr_rownnz = csr.getnnz(axis=1)\n",
    "    intrsct = csr.dot(csr.T)\n",
    "\n",
    "    nnz_i = np.repeat(csr_rownnz, intrsct.getnnz(axis=1))\n",
    "    unions = nnz_i + csr_rownnz[intrsct.indices] - intrsct.data\n",
    "    dists = 1.0 - intrsct.data / unions\n",
    "\n",
    "    mask = (dists > 0) & (dists <= epsilon)\n",
    "    data = dists[mask]\n",
    "    indices = intrsct.indices[mask]\n",
    "\n",
    "    rownnz = np.add.reduceat(mask, intrsct.indptr[:-1])\n",
    "    indptr = np.r_[0, np.cumsum(rownnz)]\n",
    "\n",
    "    out = csr_matrix((data, indices, indptr), intrsct.shape)\n",
    "    return out\n",
    "\n",
    "inds = all_data.coalesce().indices().numpy()\n",
    "\n",
    "X = csr_matrix((np.ones(inds.shape[1]), (inds[0, :], inds[1, :])), shape = (N_USERS, N_ITEMS))\n",
    "similarities = pairwise_jaccard_sparse(X, 0.87)\n",
    "similarities_symm = (similarities + similarities.T).astype(bool).astype(int)\n",
    "\n",
    "N, components = scipy.sparse.csgraph.connected_components(similarities_symm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictonary of users who have rated each item (i.e. invert train_dict)\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items.split()\n",
    "\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            test_dict[user_id] = items.split()\n",
    "        else:\n",
    "            test_dict[split[0]] = ''\n",
    "\n",
    "train_dict = {int(k):[int(id) for id in v] for k,v in train_dict.items()}\n",
    "test_dict = {int(k):[int(id) for id in v] for k,v in test_dict.items()}\n",
    "\n",
    "item_users_dict = defaultdict(lambda: set())\n",
    "for k,v in train_dict.items():\n",
    "    for x in v:\n",
    "        item_users_dict[x].add(k)\n",
    "\n",
    "for k,v in test_dict.items():\n",
    "    for x in v:\n",
    "        item_users_dict[x].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in item_users_dict.values()]).hist()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_indices = np.vstack([np.arange(len(components)), components])\n",
    "\n",
    "def get_local_neighbors(user_id):\n",
    "    return set(np.arange(N_USERS)[(similarities_symm.getrow(user_id) != 0).toarray()[0]])\n",
    "\n",
    "def get_global_neighbors(user_id):\n",
    "    return set(np.where(components == components[user_id])[0])\n",
    "    # comp_indices = np.vstack([np.arange(len(components)), components])\n",
    "    # group = components[user_id]\n",
    "    # return set(comp_indices[0, comp_indices[1, :] == group])\n",
    "\n",
    "local_neighbors_map = {}\n",
    "global_neighbors_map = {}\n",
    "for user_id in tqdm(range(N_USERS)):\n",
    "    local_neighbors_map[user_id] = get_local_neighbors(user_id)\n",
    "    global_neighbors_map[user_id] = get_global_neighbors(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cliquify_components(csr):\n",
    "    _, components = scipy.sparse.csgraph.connected_components(csr)\n",
    "\n",
    "    values = []\n",
    "    rs = []\n",
    "    cs = []\n",
    "    for i in tqdm(range(csr.shape[0])):\n",
    "        neighbors = list(np.where(components == components[i])[0])\n",
    "        neighbors.remove(i)\n",
    "        cs += neighbors\n",
    "        rs += [i] * len(neighbors)\n",
    "        values += [1] * len(neighbors)\n",
    "\n",
    "    return csr_matrix((values, (rs, cs)))\n",
    "\n",
    "global_neighbors = cliquify_components(similarities_symm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "rs = [0, 1, 1, 2, 3, 4, 4, 4, 5, 6]\n",
    "cs = [1, 0, 2, 1, 4, 3, 5, 6, 4, 4]\n",
    "vs = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "cliquify_components(csr_matrix((vs, (rs, cs)))).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in tqdm(range(N_ITEMS)):\n",
    "#     rs = list(item_users_dict[0])\n",
    "#     cs = [0] * len(rs)\n",
    "#     vs = [1] * len(rs)\n",
    "#     user_mat = csr_matrix((vs, (rs, cs)), shape = (N_USERS, 1))\n",
    "\n",
    "#     L_uis = similarities_symm @ user_mat\n",
    "#     G_uis = global_neighbors @ user_mat\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This 100% can be made much much faster with vectorization \n",
    "# but I'm too lazy to think about that at this point\n",
    "\n",
    "# Or maybe not, vectorized version still slow for some reason...\n",
    "\n",
    "if (not Path(datapath(\"item_LGRs.pickle\")).is_file()):\n",
    "    item_LGRs = []\n",
    "    item_UIS1s = []\n",
    "    for item in tqdm(range(N_ITEMS)):\n",
    "        LGRs = []\n",
    "        UIS1s = []\n",
    "        for user_id in range(N_USERS):\n",
    "            local_neighbors = local_neighbors_map[user_id]\n",
    "            global_neighbors = global_neighbors_map[user_id]\n",
    "            L_ui = len(local_neighbors & item_users_dict[item])\n",
    "            G_ui = len(global_neighbors & item_users_dict[item])\n",
    "            N_i = len(item_users_dict[item])\n",
    "\n",
    "            if (L_ui != 0 or G_ui != 0): LGRs.append(1 - L_ui / G_ui)\n",
    "            if (L_ui != 0 or N_i != 0): UIS1s.append(1 - L_ui / N_i)\n",
    "\n",
    "        item_LGRs.append(np.array(LGRs).mean())\n",
    "        item_UIS1s.append(np.array(UIS1s).mean())\n",
    "\n",
    "    with open(datapath(\"item_LGRs.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(item_LGRs, f)\n",
    "\n",
    "    with open(datapath(\"item_UIS1s.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(item_UIS1s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath(\"item_LGRs.pickle\"), \"rb\") as f:\n",
    "    item_LGRs = pickle.load(f)\n",
    "\n",
    "with open(datapath(\"item_UIS1s.pickle\"), \"rb\") as f:\n",
    "    item_UIS1s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drop in tqdm([5, 10, 15, 20, 25, 30]):\n",
    "    q = np.quantile(item_LGRs, drop * 0.01)\n",
    "    indices = np.where(item_LGRs < q)[0]\n",
    "\n",
    "    dense_train_data = train_data.bool().to_dense().clone()\n",
    "    dense_test_data = test_data.bool().to_dense().clone()\n",
    "    dense_train_data[:, indices] = 0\n",
    "    dense_test_data[:, indices] = 0\n",
    "    torch.save(dense_train_data.to_sparse(), datapath(f\"train/LGR/LGR{drop}_data.pt\"))\n",
    "    \n",
    "    test_dict = sparse_to_user_dict(dense_test_data.to_sparse())\n",
    "\n",
    "    with open(datapath(f\"test/LGR/LGR{drop}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UIS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drop in tqdm([5, 10, 15, 20, 25, 30]):\n",
    "    q = np.quantile(item_UIS1s, drop * 0.01)\n",
    "    indices = np.where(item_UIS1s < q)[0]\n",
    "\n",
    "    dense_train_data = train_data.bool().to_dense().clone()\n",
    "    dense_test_data = test_data.bool().to_dense().clone()\n",
    "    dense_train_data[:, indices] = 0\n",
    "    dense_test_data[:, indices] = 0\n",
    "    torch.save(dense_train_data.to_sparse(), datapath(f\"train/UIS1/UIS1{drop}_data.pt\"))\n",
    "    \n",
    "    test_dict = sparse_to_user_dict(dense_test_data.to_sparse())\n",
    "\n",
    "    with open(datapath(f\"test/UIS1/UIS1{drop}_test_indices.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18319"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(N_ITEMS * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "items_to_keep = np.random.choice(range(N_ITEMS), size = int(N_ITEMS_SUB), replace = False)\n",
    "item_map = {}\n",
    "for i in range(len(items_to_keep)):\n",
    "    item_map[items_to_keep[i]] = int(i)\n",
    "def subsample_users_items(sparse_train_tensor, test_dict, num_users = N_USERS_SUB, items_to_keep = items_to_keep):\n",
    "    indices = sparse_train_tensor.coalesce().indices().numpy()\n",
    "    #print(\"here1\")\n",
    "\n",
    "    indices = indices[:, (indices[0] < num_users) & np.isin(indices[1], items_to_keep)]\n",
    "    for i in range(indices.shape[1]):\n",
    "        indices[1][i] = item_map[indices[1][i]]\n",
    "    values = np.ones(indices.shape[1])\n",
    "\n",
    "    #print(\"here2\")\n",
    "\n",
    "    test_dict_out = {\n",
    "        k:\n",
    "        list(np.array(v)[np.isin(np.array(v), items_to_keep)])\n",
    "        for k,v in test_dict.items()\n",
    "    }\n",
    "    \n",
    "    for k in list(test_dict_out.keys()):\n",
    "        if k >= num_users:\n",
    "            del test_dict_out[k]\n",
    "        else:\n",
    "            new_list = [item_map[x] for x in test_dict_out[k]]\n",
    "            test_dict_out[k] = new_list\n",
    "\n",
    "    #print(test_dict_out)\n",
    "    #print(\"here3\")\n",
    "\n",
    "    return torch.sparse_coo_tensor(indices, values, size = (num_users, int(N_ITEMS_SUB))), test_dict_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top/top70\n",
      "top/top80\n",
      "top/top90\n",
      "top/top95\n",
      "middle/middle5\n",
      "middle/middle10\n",
      "middle/middle15\n",
      "middle/middle25\n",
      "uniform/uniform100\n",
      "uniform/uniform99\n",
      "uniform/uniform95\n",
      "uniform/uniform90\n",
      "uniform/uniform80\n",
      "uniform/uniform70\n",
      "uniform/uniform60\n"
     ]
    }
   ],
   "source": [
    "# TODO: add whatever sparsity levels you want to end up using\n",
    "to_modify = [\n",
    "    \"top/top70\",\n",
    "    \"top/top80\",\n",
    "    \"top/top90\",\n",
    "    \"top/top95\",\n",
    "    \"middle/middle5\",\n",
    "    \"middle/middle10\",\n",
    "    \"middle/middle15\",\n",
    "    \"middle/middle25\",\n",
    "    \"uniform/uniform100\",\n",
    "    \"uniform/uniform99\",\n",
    "    \"uniform/uniform95\",\n",
    "    \"uniform/uniform90\",\n",
    "    \"uniform/uniform80\",\n",
    "    \"uniform/uniform70\",\n",
    "    \"uniform/uniform60\",\n",
    "    \"uniform/uniform50\",\n",
    "    \"uniform/uniform40\",\n",
    "    \"LGR/LGR5\",\n",
    "    \"LGR/LGR10\",\n",
    "    \"LGR/LGR15\",\n",
    "    \"LGR/LGR25\",\n",
    "    \"UIS1/UIS15\",\n",
    "    \"UIS1/UIS110\",\n",
    "    \"UIS1/UIS115\",\n",
    "    \"UIS1/UIS125\"\n",
    "]\n",
    "\n",
    "for file_name in to_modify:\n",
    "    print(file_name)\n",
    "    sparse_train_tensor = torch.load(datapath(\"train/\" + file_name + \"_data.pt\"))\n",
    "    with open(datapath(\"test/\" + file_name + \"_test_indices.pickle\"), 'rb') as handle:\n",
    "        test_dict = pickle.load(handle)\n",
    "\n",
    "    new_train, new_test_dict = subsample_users_items(sparse_train_tensor, test_dict)\n",
    "\n",
    "    # TODO: Modify file names as you want\n",
    "    torch.save(new_train, datapath(\"train/\" + file_name + \"_data_sub.pt\"))\n",
    "    with open(datapath(\"test/\" + file_name + \"_test_indices_sub.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(new_test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x+1 , [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
