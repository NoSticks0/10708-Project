{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pysrc.constants import cachepath, datapath, chartpath\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items\n",
    "\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            test_dict[user_id] = items\n",
    "        else:\n",
    "            test_dict[split[0]] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for key in train_dict:\n",
    "    train.append([int(n) for n in train_dict[key].replace('\\n', '').split(' ')])\n",
    "\n",
    "for key in test_dict:\n",
    "    if len(test_dict[key]) == 0:\n",
    "        test.append([])\n",
    "    else:\n",
    "        test.append([int(n) for n in test_dict[key].replace('\\n', '').split(' ')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def log_likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(torch.log(yhat+1e-9) * y, axis = 1))\n",
    "\n",
    "def likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(yhat * y, axis = 1))\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims = None,\n",
    "                 kl_weight = .2\n",
    "                 ):\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        \n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [512, 128]\n",
    "            \n",
    "        self.hidden_dims = hidden_dims\n",
    "            \n",
    "        modules.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        modules.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        modules.append(nn.LeakyReLU())\n",
    "        \n",
    "        # Build Encoder\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1])\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(nn.Linear(hidden_dims[-1], input_dim),\n",
    "                                         nn.Sigmoid()\n",
    "                                        )\n",
    "        \n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, self.hidden_dims[0])\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        result = F.normalize(result, p=1)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), x, mu, log_var]\n",
    "\n",
    "    def loss_function(self, recons, x, mu, log_var) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        kld_weight = self.kl_weight\n",
    "        recons_loss = likelihood_loss(x, recons)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dim = 91599\n",
    "lr = 1e-3\n",
    "model = VanillaVAE(input_dim = item_dim, latent_dim = 64, hidden_dims = [512, 128], kl_weight = .1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_ids = np.random.choice(np.array(list(train_dict.keys())).astype(int), int(.2*len(train_dict.keys())), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_uniform(data, p_relative = .1):\n",
    "    current_sparse = None\n",
    "    batch_size = 1000\n",
    "    X = []\n",
    "    for row in data:\n",
    "        X.append(torch.zeros(item_dim).bool())\n",
    "        for item in row:\n",
    "            if np.random.random() > p_relative:\n",
    "                X[-1][item] = 1\n",
    "        X[-1] = X[-1]\n",
    "    X = torch.stack(X)\n",
    "    return X\n",
    "    \n",
    "def sparsify_items(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def sparsify_users(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def list_batch_to_ohe(data):\n",
    "    return sparsify_uniform(data, 0)\n",
    "\n",
    "dev_df = pd.Series(train).drop(test_ids)\n",
    "test_df = pd.Series(test).iloc[test_ids]\n",
    "\n",
    "train_df = dev_df.iloc[:int(.7*len(dev_df))]\n",
    "val_df = dev_df.iloc[int(.7*len(dev_df)):]\n",
    "\n",
    "train_tensor = list_batch_to_ohe(train_df)\n",
    "val_tensor = list_batch_to_ohe(val_df)\n",
    "test_tensor = list_batch_to_ohe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_tensor[:1000], batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_tensor[:1000], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_tensor[:1000], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch 0\n",
      "Train loss: 5.100347518920898\n",
      "Train r loss: -0.0011117624817416072\n",
      "Train kl loss: -0.2550729513168335\n",
      "Val loss: 2.993506669998169\n",
      "Val r loss: -0.00040399233694188297\n",
      "Val kl loss: -0.14969554543495178\n",
      "Start Epoch 1\n",
      "Train loss: 13.940979957580566\n",
      "Train r loss: -0.001133727259002626\n",
      "Train kl loss: -0.6971057057380676\n",
      "Val loss: 2.0622518062591553\n",
      "Val r loss: -0.00040466393693350255\n",
      "Val kl loss: -0.10313281416893005\n",
      "Start Epoch 2\n",
      "Train loss: 19.27093505859375\n",
      "Train r loss: -0.0011439737863838673\n",
      "Train kl loss: -0.9636039137840271\n",
      "Val loss: 1.4896208047866821\n",
      "Val r loss: -0.00040358921978622675\n",
      "Val kl loss: -0.07450122386217117\n",
      "Start Epoch 3\n",
      "Train loss: 6.002903938293457\n",
      "Train r loss: -0.0011505676666274667\n",
      "Train kl loss: -0.30020269751548767\n",
      "Val loss: 1.0617668628692627\n",
      "Val r loss: -0.000403337471652776\n",
      "Val kl loss: -0.05310850962996483\n",
      "Start Epoch 4\n",
      "Train loss: 5.414379119873047\n",
      "Train r loss: -0.0011587712215259671\n",
      "Train kl loss: -0.2707768678665161\n",
      "Val loss: 0.8686552047729492\n",
      "Val r loss: -0.0004037891048938036\n",
      "Val kl loss: -0.043452948331832886\n",
      "Start Epoch 5\n",
      "Train loss: 3.177365779876709\n",
      "Train r loss: -0.001166393281891942\n",
      "Train kl loss: -0.1589266061782837\n",
      "Val loss: 0.7052062749862671\n",
      "Val r loss: -0.0004031045245938003\n",
      "Val kl loss: -0.03528046980500221\n",
      "Start Epoch 6\n",
      "Train loss: 2.8672432899475098\n",
      "Train r loss: -0.00117746542673558\n",
      "Train kl loss: -0.14342103898525238\n",
      "Val loss: 0.42023053765296936\n",
      "Val r loss: -0.0004042132932227105\n",
      "Val kl loss: -0.021031735464930534\n",
      "Start Epoch 7\n",
      "Train loss: 2.2386634349823\n",
      "Train r loss: -0.0011816282058134675\n",
      "Train kl loss: -0.1119922548532486\n",
      "Val loss: 0.46389031410217285\n",
      "Val r loss: -0.00040519534377381206\n",
      "Val kl loss: -0.02321477420628071\n",
      "Start Epoch 8\n",
      "Train loss: 1.7315165996551514\n",
      "Train r loss: -0.0011756103485822678\n",
      "Train kl loss: -0.08663460612297058\n",
      "Val loss: 0.3775990307331085\n",
      "Val r loss: -0.0004024218360427767\n",
      "Val kl loss: -0.018900074064731598\n",
      "Start Epoch 9\n",
      "Train loss: 1.438501000404358\n",
      "Train r loss: -0.001174108823761344\n",
      "Train kl loss: -0.07198374718427658\n",
      "Val loss: 0.3508649468421936\n",
      "Val r loss: -0.0004023334477096796\n",
      "Val kl loss: -0.017563363537192345\n",
      "Start Epoch 10\n",
      "Train loss: 1.158811092376709\n",
      "Train r loss: -0.0011921009281650186\n",
      "Train kl loss: -0.05800016596913338\n",
      "Val loss: 0.29617494344711304\n",
      "Val r loss: -0.00040395456016995013\n",
      "Val kl loss: -0.014828944578766823\n",
      "Start Epoch 11\n",
      "Train loss: 1.052247166633606\n",
      "Train r loss: -0.0012041173176839948\n",
      "Train kl loss: -0.05267256125807762\n",
      "Val loss: 0.33049073815345764\n",
      "Val r loss: -0.0004032314463984221\n",
      "Val kl loss: -0.016544697806239128\n",
      "Start Epoch 12\n",
      "Train loss: 0.8699235320091248\n",
      "Train r loss: -0.0012126073706895113\n",
      "Train kl loss: -0.043556805700063705\n",
      "Val loss: 0.2890256941318512\n",
      "Val r loss: -0.00040212919702753425\n",
      "Val kl loss: -0.014471392147243023\n",
      "Start Epoch 13\n",
      "Train loss: 0.7546214461326599\n",
      "Train r loss: -0.0012098121223971248\n",
      "Train kl loss: -0.03779156506061554\n",
      "Val loss: 0.25014975666999817\n",
      "Val r loss: -0.0004003348294645548\n",
      "Val kl loss: -0.012527505867183208\n",
      "Start Epoch 14\n",
      "Train loss: 0.7433234453201294\n",
      "Train r loss: -0.0012281283270567656\n",
      "Train kl loss: -0.0372275784611702\n",
      "Val loss: 0.31168249249458313\n",
      "Val r loss: -0.00040315280784852803\n",
      "Val kl loss: -0.015604281798005104\n",
      "Start Epoch 15\n",
      "Train loss: 0.6419249773025513\n",
      "Train r loss: -0.0011913253692910075\n",
      "Train kl loss: -0.03215581551194191\n",
      "Val loss: 0.2779434025287628\n",
      "Val r loss: -0.0004035429155919701\n",
      "Val kl loss: -0.0139173474162817\n",
      "Start Epoch 16\n",
      "Train loss: 0.6226194500923157\n",
      "Train r loss: -0.0011941618286073208\n",
      "Train kl loss: -0.03119068033993244\n",
      "Val loss: 0.3360157907009125\n",
      "Val r loss: -0.0004010290722362697\n",
      "Val kl loss: -0.016820840537548065\n",
      "Start Epoch 17\n",
      "Train loss: 0.5437154769897461\n",
      "Train r loss: -0.0011917087249457836\n",
      "Train kl loss: -0.027245359495282173\n",
      "Val loss: 0.25357484817504883\n",
      "Val r loss: -0.0004036161699332297\n",
      "Val kl loss: -0.012698923237621784\n",
      "Start Epoch 18\n",
      "Train loss: 0.5480406284332275\n",
      "Train r loss: -0.00118105742149055\n",
      "Train kl loss: -0.027461083605885506\n",
      "Val loss: 0.30520033836364746\n",
      "Val r loss: -0.00040245329728350043\n",
      "Val kl loss: -0.015280138701200485\n",
      "Start Epoch 19\n",
      "Train loss: 0.5037685036659241\n",
      "Train r loss: -0.0011952498462051153\n",
      "Train kl loss: -0.02524818666279316\n",
      "Val loss: 0.25916844606399536\n",
      "Val r loss: -0.00040284075657837093\n",
      "Val kl loss: -0.012978564947843552\n",
      "Start Epoch 20\n",
      "Train loss: 0.4836775064468384\n",
      "Train r loss: -0.0011832932941615582\n",
      "Train kl loss: -0.02424304001033306\n",
      "Val loss: 0.26237234473228455\n",
      "Val r loss: -0.00040259910747408867\n",
      "Val kl loss: -0.013138746842741966\n",
      "Start Epoch 21\n",
      "Train loss: 0.4853484332561493\n",
      "Train r loss: -0.00119260314386338\n",
      "Train kl loss: -0.024327050894498825\n",
      "Val loss: 0.20375901460647583\n",
      "Val r loss: -0.0004015167069155723\n",
      "Val kl loss: -0.010208025574684143\n",
      "Start Epoch 22\n",
      "Train loss: 0.42217567563056946\n",
      "Train r loss: -0.0011776505270972848\n",
      "Train kl loss: -0.021167665719985962\n",
      "Val loss: 0.20143920183181763\n",
      "Val r loss: -0.00040402039303444326\n",
      "Val kl loss: -0.010092162527143955\n",
      "Start Epoch 23\n",
      "Train loss: 0.41637638211250305\n",
      "Train r loss: -0.0011748868273571134\n",
      "Train kl loss: -0.02087756246328354\n",
      "Val loss: 0.2121778279542923\n",
      "Val r loss: -0.000404350837925449\n",
      "Val kl loss: -0.010629109106957912\n",
      "Start Epoch 24\n",
      "Train loss: 0.36838585138320923\n",
      "Train r loss: -0.0011773145524784923\n",
      "Train kl loss: -0.018478158861398697\n",
      "Val loss: 0.17295709252357483\n",
      "Val r loss: -0.00040359460399486125\n",
      "Val kl loss: -0.008668034337460995\n",
      "Start Epoch 25\n",
      "Train loss: 0.34612467885017395\n",
      "Train r loss: -0.0011682725744321942\n",
      "Train kl loss: -0.01736464537680149\n",
      "Val loss: 0.17075464129447937\n",
      "Val r loss: -0.000401476863771677\n",
      "Val kl loss: -0.00855780579149723\n",
      "Start Epoch 26\n",
      "Train loss: 0.3779720067977905\n",
      "Train r loss: -0.0011877743527293205\n",
      "Train kl loss: -0.018957991153001785\n",
      "Val loss: 0.22051754593849182\n",
      "Val r loss: -0.00040169002022594213\n",
      "Val kl loss: -0.011045961640775204\n",
      "Start Epoch 27\n",
      "Train loss: 0.31734898686408997\n",
      "Train r loss: -0.0011887035798281431\n",
      "Train kl loss: -0.01592688448727131\n",
      "Val loss: 0.1732567548751831\n",
      "Val r loss: -0.0004021474742330611\n",
      "Val kl loss: -0.008682945743203163\n",
      "Start Epoch 28\n",
      "Train loss: 0.30912917852401733\n",
      "Train r loss: -0.001178520149551332\n",
      "Train kl loss: -0.015515384264290333\n",
      "Val loss: 0.18596382439136505\n",
      "Val r loss: -0.0004007074749097228\n",
      "Val kl loss: -0.009318226017057896\n",
      "Start Epoch 29\n",
      "Train loss: 0.30191048979759216\n",
      "Train r loss: -0.0011686988873407245\n",
      "Train kl loss: -0.015153960324823856\n",
      "Val loss: 0.17169725894927979\n",
      "Val r loss: -0.0004033645091112703\n",
      "Val kl loss: -0.008605031296610832\n",
      "Start Epoch 30\n",
      "Train loss: 0.2877379357814789\n",
      "Train r loss: -0.0011602615704759955\n",
      "Train kl loss: -0.014444909058511257\n",
      "Val loss: 0.1528303325176239\n",
      "Val r loss: -0.00040545500814914703\n",
      "Val kl loss: -0.007661789655685425\n",
      "Start Epoch 31\n",
      "Train loss: 0.30522575974464417\n",
      "Train r loss: -0.0011729393154382706\n",
      "Train kl loss: -0.015319935977458954\n",
      "Val loss: 0.20519398152828217\n",
      "Val r loss: -0.0004034570010844618\n",
      "Val kl loss: -0.010279872454702854\n",
      "Start Epoch 32\n",
      "Train loss: 0.247024804353714\n",
      "Train r loss: -0.00116908666677773\n",
      "Train kl loss: -0.012409692630171776\n",
      "Val loss: 0.1381111443042755\n",
      "Val r loss: -0.0004027037648484111\n",
      "Val kl loss: -0.0069256918504834175\n",
      "Start Epoch 33\n",
      "Train loss: 0.26330816745758057\n",
      "Train r loss: -0.0011717003071680665\n",
      "Train kl loss: -0.013223993591964245\n",
      "Val loss: 0.1506074219942093\n",
      "Val r loss: -0.00040157753392122686\n",
      "Val kl loss: -0.007550450041890144\n",
      "Start Epoch 34\n",
      "Train loss: 0.28211262822151184\n",
      "Train r loss: -0.0011775015154853463\n",
      "Train kl loss: -0.014164507389068604\n",
      "Val loss: 0.17625777423381805\n",
      "Val r loss: -0.0004035186138935387\n",
      "Val kl loss: -0.008833064697682858\n",
      "Start Epoch 35\n",
      "Train loss: 0.2643904685974121\n",
      "Train r loss: -0.0011779065243899822\n",
      "Train kl loss: -0.013278418220579624\n",
      "Val loss: 0.16361592710018158\n",
      "Val r loss: -0.0004020380729343742\n",
      "Val kl loss: -0.008200899697840214\n",
      "Start Epoch 36\n",
      "Train loss: 0.25987619161605835\n",
      "Train r loss: -0.0011772880097851157\n",
      "Train kl loss: -0.013052674941718578\n",
      "Val loss: 0.16873279213905334\n",
      "Val r loss: -0.0004034649464301765\n",
      "Val kl loss: -0.008456813171505928\n",
      "Start Epoch 37\n",
      "Train loss: 0.22909460961818695\n",
      "Train r loss: -0.0011667251819744706\n",
      "Train kl loss: -0.011513067409396172\n",
      "Val loss: 0.13466787338256836\n",
      "Val r loss: -0.00040228263242170215\n",
      "Val kl loss: -0.006753508001565933\n",
      "Start Epoch 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2136102318763733\n",
      "Train r loss: -0.0011687285732477903\n",
      "Train kl loss: -0.010738948360085487\n",
      "Val loss: 0.12759961187839508\n",
      "Val r loss: -0.0004042536311317235\n",
      "Val kl loss: -0.006400193087756634\n",
      "Start Epoch 39\n",
      "Train loss: 0.2002469003200531\n",
      "Train r loss: -0.001180416438728571\n",
      "Train kl loss: -0.010071365162730217\n",
      "Val loss: 0.09556138515472412\n",
      "Val r loss: -0.0004048848641104996\n",
      "Val kl loss: -0.00479831313714385\n",
      "Start Epoch 40\n",
      "Train loss: 0.2137344479560852\n",
      "Train r loss: -0.0011722969356924295\n",
      "Train kl loss: -0.01074533723294735\n",
      "Val loss: 0.12533162534236908\n",
      "Val r loss: -0.00040362091385759413\n",
      "Val kl loss: -0.006286762189120054\n",
      "Start Epoch 41\n",
      "Train loss: 0.2119370698928833\n",
      "Train r loss: -0.0011556977406144142\n",
      "Train kl loss: -0.010654637590050697\n",
      "Val loss: 0.13179220259189606\n",
      "Val r loss: -0.0004033217846881598\n",
      "Val kl loss: -0.006609776522964239\n",
      "Start Epoch 42\n",
      "Train loss: 0.20523425936698914\n",
      "Train r loss: -0.0011902806581929326\n",
      "Train kl loss: -0.010321227833628654\n",
      "Val loss: 0.12822061777114868\n",
      "Val r loss: -0.00040266875294037163\n",
      "Val kl loss: -0.006431164685636759\n",
      "Start Epoch 43\n",
      "Train loss: 0.1952829360961914\n",
      "Train r loss: -0.0011552686337381601\n",
      "Train kl loss: -0.009821910411119461\n",
      "Val loss: 0.12116802483797073\n",
      "Val r loss: -0.00040330999763682485\n",
      "Val kl loss: -0.006078566890209913\n",
      "Start Epoch 44\n",
      "Train loss: 0.18657425045967102\n",
      "Train r loss: -0.0011802674271166325\n",
      "Train kl loss: -0.009387725032866001\n",
      "Val loss: 0.09912384301424026\n",
      "Val r loss: -0.0004052230215165764\n",
      "Val kl loss: -0.004976452793926001\n",
      "Start Epoch 45\n",
      "Train loss: 0.24744148552417755\n",
      "Train r loss: -0.001164018176496029\n",
      "Train kl loss: -0.012430273927748203\n",
      "Val loss: 0.19028450548648834\n",
      "Val r loss: -0.0004026208189316094\n",
      "Val kl loss: -0.009534355252981186\n",
      "Start Epoch 46\n",
      "Train loss: 0.18179281055927277\n",
      "Train r loss: -0.0011819247156381607\n",
      "Train kl loss: -0.009148737415671349\n",
      "Val loss: 0.10824601352214813\n",
      "Val r loss: -0.00040330615593120456\n",
      "Val kl loss: -0.005432466510683298\n",
      "Start Epoch 47\n",
      "Train loss: 0.16848209500312805\n",
      "Train r loss: -0.0011638845317065716\n",
      "Train kl loss: -0.008482299745082855\n",
      "Val loss: 0.11241713166236877\n",
      "Val r loss: -0.0004039381165057421\n",
      "Val kl loss: -0.005641053430736065\n",
      "Start Epoch 48\n",
      "Train loss: 0.16241061687469482\n",
      "Train r loss: -0.0011630605440586805\n",
      "Train kl loss: -0.008178683929145336\n",
      "Val loss: 0.0987938717007637\n",
      "Val r loss: -0.0004029319388791919\n",
      "Val kl loss: -0.004959840793162584\n",
      "Start Epoch 49\n",
      "Train loss: 0.16978441178798676\n",
      "Train r loss: -0.0011525278678163886\n",
      "Train kl loss: -0.008546845987439156\n",
      "Val loss: 0.11179361492395401\n",
      "Val r loss: -0.0004036489990539849\n",
      "Val kl loss: -0.0056098634377121925\n",
      "Start Epoch 50\n",
      "Train loss: 0.16099777817726135\n",
      "Train r loss: -0.0011623657774180174\n",
      "Train kl loss: -0.008108006790280342\n",
      "Val loss: 0.09700872749090195\n",
      "Val r loss: -0.00040436015115119517\n",
      "Val kl loss: -0.004870654549449682\n",
      "Start Epoch 51\n",
      "Train loss: 0.15380193293094635\n",
      "Train r loss: -0.0011557069374248385\n",
      "Train kl loss: -0.007747882045805454\n",
      "Val loss: 0.09405726939439774\n",
      "Val r loss: -0.00040459894808009267\n",
      "Val kl loss: -0.004723093006759882\n",
      "Start Epoch 52\n",
      "Train loss: 0.17559519410133362\n",
      "Train r loss: -0.001160594285465777\n",
      "Train kl loss: -0.008837789297103882\n",
      "Val loss: 0.13634826242923737\n",
      "Val r loss: -0.00040459283627569675\n",
      "Val kl loss: -0.006837642751634121\n",
      "Start Epoch 53\n",
      "Train loss: 0.13325543701648712\n",
      "Train r loss: -0.001148813869804144\n",
      "Train kl loss: -0.006720211822539568\n",
      "Val loss: 0.07517030090093613\n",
      "Val r loss: -0.00040369475027546287\n",
      "Val kl loss: -0.0037786997854709625\n",
      "Start Epoch 54\n",
      "Train loss: 0.20775680243968964\n",
      "Train r loss: -0.0011511963093653321\n",
      "Train kl loss: -0.010445399209856987\n",
      "Val loss: 0.16702032089233398\n",
      "Val r loss: -0.0004028809489682317\n",
      "Val kl loss: -0.008371160365641117\n",
      "Start Epoch 55\n",
      "Train loss: 0.15470536053180695\n",
      "Train r loss: -0.001154526835307479\n",
      "Train kl loss: -0.007792994379997253\n",
      "Val loss: 0.09666702151298523\n",
      "Val r loss: -0.0004043867520522326\n",
      "Val kl loss: -0.004853569902479649\n",
      "Start Epoch 56\n",
      "Train loss: 0.18801864981651306\n",
      "Train r loss: -0.001157413236796856\n",
      "Train kl loss: -0.009458803571760654\n",
      "Val loss: 0.14873187243938446\n",
      "Val r loss: -0.00040232151513919234\n",
      "Val kl loss: -0.007456710562109947\n",
      "Start Epoch 57\n",
      "Train loss: 0.13842050731182098\n",
      "Train r loss: -0.001143898000009358\n",
      "Train kl loss: -0.00697822030633688\n",
      "Val loss: 0.08385759592056274\n",
      "Val r loss: -0.00040338817052543163\n",
      "Val kl loss: -0.004213049076497555\n",
      "Start Epoch 58\n",
      "Train loss: 0.1342533379793167\n",
      "Train r loss: -0.0011504552094265819\n",
      "Train kl loss: -0.006770189851522446\n",
      "Val loss: 0.0935782790184021\n",
      "Val r loss: -0.00040437333518639207\n",
      "Val kl loss: -0.004699133336544037\n",
      "Start Epoch 59\n",
      "Train loss: 0.156061053276062\n",
      "Train r loss: -0.0011478376109153032\n",
      "Train kl loss: -0.007860444486141205\n",
      "Val loss: 0.11586499214172363\n",
      "Val r loss: -0.00040446181083098054\n",
      "Val kl loss: -0.005813472904264927\n",
      "Start Epoch 60\n",
      "Train loss: 0.1598537266254425\n",
      "Train r loss: -0.001152690383605659\n",
      "Train kl loss: -0.008050321601331234\n",
      "Val loss: 0.10625244677066803\n",
      "Val r loss: -0.0004037036560475826\n",
      "Val kl loss: -0.005332807078957558\n",
      "Start Epoch 61\n",
      "Train loss: 0.14214961230754852\n",
      "Train r loss: -0.0011699504684656858\n",
      "Train kl loss: -0.007165978197008371\n",
      "Val loss: 0.09235472977161407\n",
      "Val r loss: -0.0004034846497233957\n",
      "Val kl loss: -0.004637910518795252\n",
      "Start Epoch 62\n",
      "Train loss: 0.15949490666389465\n",
      "Train r loss: -0.0011465217685326934\n",
      "Train kl loss: -0.008032072335481644\n",
      "Val loss: 0.10276857018470764\n",
      "Val r loss: -0.0004044375382363796\n",
      "Val kl loss: -0.0051586502231657505\n",
      "Start Epoch 63\n",
      "Train loss: 0.1382373869419098\n",
      "Train r loss: -0.001143758650869131\n",
      "Train kl loss: -0.006969057954847813\n",
      "Val loss: 0.08901272714138031\n",
      "Val r loss: -0.0004044353263452649\n",
      "Val kl loss: -0.004470858257263899\n",
      "Start Epoch 64\n",
      "Train loss: 0.12348020821809769\n",
      "Train r loss: -0.0011488470481708646\n",
      "Train kl loss: -0.00623145280405879\n",
      "Val loss: 0.06848829984664917\n",
      "Val r loss: -0.0004046107060275972\n",
      "Val kl loss: -0.0034446450881659985\n",
      "Start Epoch 65\n",
      "Train loss: 0.14009754359722137\n",
      "Train r loss: -0.001172465505078435\n",
      "Train kl loss: -0.00706349965184927\n",
      "Val loss: 0.10866426676511765\n",
      "Val r loss: -0.0004024475929327309\n",
      "Val kl loss: -0.0054533351212739944\n",
      "Start Epoch 66\n",
      "Train loss: 0.12813515961170197\n",
      "Train r loss: -0.0011438415385782719\n",
      "Train kl loss: -0.006463950499892235\n",
      "Val loss: 0.07610300183296204\n",
      "Val r loss: -0.00040448992513120174\n",
      "Val kl loss: -0.00382537511177361\n",
      "Start Epoch 67\n",
      "Train loss: 0.1250646710395813\n",
      "Train r loss: -0.0011392979649826884\n",
      "Train kl loss: -0.0063101984560489655\n",
      "Val loss: 0.07577499747276306\n",
      "Val r loss: -0.00040321765118278563\n",
      "Val kl loss: -0.0038089104928076267\n",
      "Start Epoch 68\n",
      "Train loss: 0.13236193358898163\n",
      "Train r loss: -0.0011452969629317522\n",
      "Train kl loss: -0.006675361189991236\n",
      "Val loss: 0.0921437069773674\n",
      "Val r loss: -0.0004040728963445872\n",
      "Val kl loss: -0.0046273889020085335\n",
      "Start Epoch 69\n",
      "Train loss: 0.10362804681062698\n",
      "Train r loss: -0.0011460387613624334\n",
      "Train kl loss: -0.005238704849034548\n",
      "Val loss: 0.062396302819252014\n",
      "Val r loss: -0.00040635038749314845\n",
      "Val kl loss: -0.0031401326414197683\n",
      "Start Epoch 70\n",
      "Train loss: 0.11304261535406113\n",
      "Train r loss: -0.001148529932834208\n",
      "Train kl loss: -0.005709557794034481\n",
      "Val loss: 0.07586129754781723\n",
      "Val r loss: -0.00040449982043355703\n",
      "Val kl loss: -0.0038132898043841124\n",
      "Start Epoch 71\n",
      "Train loss: 0.1175786554813385\n",
      "Train r loss: -0.001147662871517241\n",
      "Train kl loss: -0.0059363157488405704\n",
      "Val loss: 0.08836580067873001\n",
      "Val r loss: -0.00040365595486946404\n",
      "Val kl loss: -0.0044384729117155075\n",
      "Start Epoch 72\n",
      "Train loss: 0.1186000406742096\n",
      "Train r loss: -0.001138069899752736\n",
      "Train kl loss: -0.005986904725432396\n",
      "Val loss: 0.08618144690990448\n",
      "Val r loss: -0.00040489196544513106\n",
      "Val kl loss: -0.0043293172493577\n",
      "Start Epoch 73\n",
      "Train loss: 0.12001487612724304\n",
      "Train r loss: -0.0011393546592444181\n",
      "Train kl loss: -0.0060577113181352615\n",
      "Val loss: 0.08124303817749023\n",
      "Val r loss: -0.00040393671952188015\n",
      "Val kl loss: -0.0040823486633598804\n",
      "Start Epoch 74\n",
      "Train loss: 0.10815809667110443\n",
      "Train r loss: -0.001138485036790371\n",
      "Train kl loss: -0.005464829504489899\n",
      "Val loss: 0.07613038271665573\n",
      "Val r loss: -0.00040363919106312096\n",
      "Val kl loss: -0.0038267013151198626\n",
      "Start Epoch 75\n",
      "Train loss: 0.12115710973739624\n",
      "Train r loss: -0.0011447984725236893\n",
      "Train kl loss: -0.006115095689892769\n",
      "Val loss: 0.09568138420581818\n",
      "Val r loss: -0.00040405942127108574\n",
      "Val kl loss: -0.0048042722046375275\n",
      "Start Epoch 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10146117955446243\n",
      "Train r loss: -0.001137272920459509\n",
      "Train kl loss: -0.005129923112690449\n",
      "Val loss: 0.06831273436546326\n",
      "Val r loss: -0.000402811449021101\n",
      "Val kl loss: -0.003435777500271797\n",
      "Start Epoch 77\n",
      "Train loss: 0.10144151747226715\n",
      "Train r loss: -0.0011381900403648615\n",
      "Train kl loss: -0.005128985736519098\n",
      "Val loss: 0.06739243865013123\n",
      "Val r loss: -0.00040380944847129285\n",
      "Val kl loss: -0.0033898120746016502\n",
      "Start Epoch 78\n",
      "Train loss: 0.10807477682828903\n",
      "Train r loss: -0.0011391740990802646\n",
      "Train kl loss: -0.00546069722622633\n",
      "Val loss: 0.08194286376237869\n",
      "Val r loss: -0.000403963727876544\n",
      "Val kl loss: -0.004117341246455908\n",
      "Start Epoch 79\n",
      "Train loss: 0.10755914449691772\n",
      "Train r loss: -0.00113389000762254\n",
      "Train kl loss: -0.005434651859104633\n",
      "Val loss: 0.07999880611896515\n",
      "Val r loss: -0.00040637870552018285\n",
      "Val kl loss: -0.004020259249955416\n",
      "Start Epoch 80\n",
      "Train loss: 0.09492988884449005\n",
      "Train r loss: -0.0011642595054581761\n",
      "Train kl loss: -0.004804707132279873\n",
      "Val loss: 0.06080994755029678\n",
      "Val r loss: -0.000404192105634138\n",
      "Val kl loss: -0.003060707123950124\n",
      "Start Epoch 81\n",
      "Train loss: 0.12540768086910248\n",
      "Train r loss: -0.001137909130193293\n",
      "Train kl loss: -0.006327278912067413\n",
      "Val loss: 0.10674671083688736\n",
      "Val r loss: -0.00040430735680274665\n",
      "Val kl loss: -0.005357550457119942\n",
      "Start Epoch 82\n",
      "Train loss: 0.13249464333057404\n",
      "Train r loss: -0.0011337099131196737\n",
      "Train kl loss: -0.0066814180463552475\n",
      "Val loss: 0.11742892861366272\n",
      "Val r loss: -0.0004037728067487478\n",
      "Val kl loss: -0.0058916350826621056\n",
      "Start Epoch 83\n",
      "Train loss: 0.12845656275749207\n",
      "Train r loss: -0.001130874501541257\n",
      "Train kl loss: -0.006479371339082718\n",
      "Val loss: 0.11229879409074783\n",
      "Val r loss: -0.0004038871265947819\n",
      "Val kl loss: -0.005635133944451809\n",
      "Start Epoch 84\n",
      "Train loss: 0.10368213802576065\n",
      "Train r loss: -0.0011361190117895603\n",
      "Train kl loss: -0.005240913480520248\n",
      "Val loss: 0.08127714693546295\n",
      "Val r loss: -0.0004057420010212809\n",
      "Val kl loss: -0.004084144253283739\n",
      "Start Epoch 85\n",
      "Train loss: 0.15193574130535126\n",
      "Train r loss: -0.0011367637198418379\n",
      "Train kl loss: -0.007653625216335058\n",
      "Val loss: 0.14507319033145905\n",
      "Val r loss: -0.0004052958101965487\n",
      "Val kl loss: -0.00727392453700304\n",
      "Start Epoch 86\n",
      "Train loss: 0.10082191228866577\n",
      "Train r loss: -0.0011312004644423723\n",
      "Train kl loss: -0.005097655579447746\n",
      "Val loss: 0.07468602806329727\n",
      "Val r loss: -0.00040337422979064286\n",
      "Val kl loss: -0.0037544702645391226\n",
      "Start Epoch 87\n",
      "Train loss: 0.12087944149971008\n",
      "Train r loss: -0.0011495138751342893\n",
      "Train kl loss: -0.006101447623223066\n",
      "Val loss: 0.09778603911399841\n",
      "Val r loss: -0.0004038271144963801\n",
      "Val kl loss: -0.0049094934947788715\n",
      "Start Epoch 88\n",
      "Train loss: 0.10839729756116867\n",
      "Train r loss: -0.0011311243288218975\n",
      "Train kl loss: -0.005476420745253563\n",
      "Val loss: 0.0841292217373848\n",
      "Val r loss: -0.00040258164517581463\n",
      "Val kl loss: -0.0042265900410711765\n",
      "Start Epoch 89\n",
      "Train loss: 0.12972119450569153\n",
      "Train r loss: -0.0011283843778073788\n",
      "Train kl loss: -0.006542478688061237\n",
      "Val loss: 0.12261759489774704\n",
      "Val r loss: -0.0004042774089612067\n",
      "Val kl loss: -0.006151093635708094\n",
      "Start Epoch 90\n",
      "Train loss: 0.10696697235107422\n",
      "Train r loss: -0.0011287546949461102\n",
      "Train kl loss: -0.0054047866724431515\n",
      "Val loss: 0.08572269976139069\n",
      "Val r loss: -0.0004039495252072811\n",
      "Val kl loss: -0.00430633220821619\n",
      "Start Epoch 91\n",
      "Train loss: 0.1378508359193802\n",
      "Train r loss: -0.0011299451580271125\n",
      "Train kl loss: -0.006949039176106453\n",
      "Val loss: 0.12504634261131287\n",
      "Val r loss: -0.0004040185594931245\n",
      "Val kl loss: -0.0062725176103413105\n",
      "Start Epoch 92\n",
      "Train loss: 0.11471085250377655\n",
      "Train r loss: -0.0011267558438703418\n",
      "Train kl loss: -0.00579187972471118\n",
      "Val loss: 0.10442878305912018\n",
      "Val r loss: -0.0004044712404720485\n",
      "Val kl loss: -0.005241661798208952\n",
      "Start Epoch 93\n",
      "Train loss: 0.10399920493364334\n",
      "Train r loss: -0.0011223341571167111\n",
      "Train kl loss: -0.00525607680901885\n",
      "Val loss: 0.08619701117277145\n",
      "Val r loss: -0.0004046950489282608\n",
      "Val kl loss: -0.004330085124820471\n",
      "Start Epoch 94\n",
      "Train loss: 0.0983571857213974\n",
      "Train r loss: -0.0011270875111222267\n",
      "Train kl loss: -0.004974213894456625\n",
      "Val loss: 0.06920953094959259\n",
      "Val r loss: -0.0004038555780425668\n",
      "Val kl loss: -0.0034806695766747\n",
      "Start Epoch 95\n",
      "Train loss: 0.10554368048906326\n",
      "Train r loss: -0.0011308548273518682\n",
      "Train kl loss: -0.0053337267599999905\n",
      "Val loss: 0.08479046821594238\n",
      "Val r loss: -0.00040407790220342577\n",
      "Val kl loss: -0.004259726498275995\n",
      "Start Epoch 96\n",
      "Train loss: 0.09843627363443375\n",
      "Train r loss: -0.0011218709405511618\n",
      "Train kl loss: -0.004977907054126263\n",
      "Val loss: 0.08044017851352692\n",
      "Val r loss: -0.00040477054426446557\n",
      "Val kl loss: -0.0040422473102808\n",
      "Start Epoch 97\n",
      "Train loss: 0.08913592994213104\n",
      "Train r loss: -0.001122994115576148\n",
      "Train kl loss: -0.004512946121394634\n",
      "Val loss: 0.06750902533531189\n",
      "Val r loss: -0.00040516594890505075\n",
      "Val kl loss: -0.0033957099076360464\n",
      "Start Epoch 98\n",
      "Train loss: 0.08137775957584381\n",
      "Train r loss: -0.00115098780952394\n",
      "Train kl loss: -0.00412643700838089\n",
      "Val loss: 0.054769910871982574\n",
      "Val r loss: -0.0004038022889290005\n",
      "Val kl loss: -0.0027586854994297028\n",
      "Start Epoch 99\n",
      "Train loss: 0.09663760662078857\n",
      "Train r loss: -0.0011440329253673553\n",
      "Train kl loss: -0.004889082163572311\n",
      "Val loss: 0.07428338378667831\n",
      "Val r loss: -0.0004039413179270923\n",
      "Val kl loss: -0.003734366036951542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Start Epoch {epoch}\")\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loading_time = 0\n",
    "    for batch_X in train_dataloader:\n",
    "        batch_X = batch_X.float()\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        train_loss = model.loss_function(recons, x, mu, log_var)['loss']\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    r_train_loss = 0\n",
    "    kl_train_loss = 0\n",
    "    train_batches = 0\n",
    "    for batch_X in train_dataloader:\n",
    "        batch_X = batch_X.float()\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        loss = model.loss_function(recons, x, mu, log_var)\n",
    "        total_train_loss += loss['loss']\n",
    "        r_train_loss += loss['Reconstruction_Loss']\n",
    "        kl_train_loss += loss['KLD']\n",
    "        train_batches += 1\n",
    "        \n",
    "    total_train_loss /= train_batches\n",
    "    r_train_loss /= train_batches\n",
    "    kl_train_loss /= train_batches\n",
    "    \n",
    "    total_loss = 0\n",
    "    r_loss = 0\n",
    "    kl_loss = 0\n",
    "    batches = 0\n",
    "    for batch_X in val_dataloader:\n",
    "        batch_X = batch_X.float()\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        loss = model.loss_function(recons, x, mu, log_var)\n",
    "        total_loss += loss['loss']\n",
    "        r_loss += loss['Reconstruction_Loss']\n",
    "        kl_loss += loss['KLD']\n",
    "        batches += 1\n",
    "    \n",
    "    total_loss /= batches\n",
    "    r_loss /= batches\n",
    "    kl_loss /= batches\n",
    "    \n",
    "    print(f'Train loss: {total_train_loss}')\n",
    "    print(f'Train r loss: {r_train_loss}')\n",
    "    print(f'Train kl loss: {kl_train_loss}')\n",
    "    \n",
    "    print(f'Val loss: {total_loss}')\n",
    "    print(f'Val r loss: {r_loss}')\n",
    "    print(f'Val kl loss: {kl_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d476dfdcdfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_batch_to_ohe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_loading_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mload_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrecons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "batch_train = train_df[i * batch_size: min(len(train_df)-1, (i+1) * batch_size)]\n",
    "batch_X, load_time = list_batch_to_ohe(batch_train)\n",
    "total_loading_time += load_time\n",
    "recons, x, mu, log_var = model.forward(batch_X)\n",
    "train_loss = model.loss_function(recons, x, mu, log_var)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(-449.1120, grad_fn=<AddBackward0>),\n",
       " 'Reconstruction_Loss': tensor(-450.7499),\n",
       " 'KLD': tensor(-8.1898)}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_function(recons, x, mu, log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "pd.Series(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n"
     ]
    }
   ],
   "source": [
    "items = set()\n",
    "for key in train_dict:\n",
    "    if int(key) % 1000 == 0:\n",
    "        print(key)\n",
    "    \n",
    "    items = items.union(items, set(train_dict[key].replace('\\n', '').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91599"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'43979',\n",
       " '39065',\n",
       " '33205',\n",
       " '89792',\n",
       " '30523',\n",
       " '70033',\n",
       " '88878',\n",
       " '41347',\n",
       " '68344',\n",
       " '86949',\n",
       " '44103',\n",
       " '2951',\n",
       " '85171',\n",
       " '60160',\n",
       " '11377',\n",
       " '72298',\n",
       " '20399',\n",
       " '5311',\n",
       " '58242',\n",
       " '54536',\n",
       " '57125',\n",
       " '87479',\n",
       " '87537',\n",
       " '12518',\n",
       " '2327',\n",
       " '7306',\n",
       " '78055',\n",
       " '72185',\n",
       " '6584',\n",
       " '40171',\n",
       " '50978',\n",
       " '42484',\n",
       " '72646',\n",
       " '82726',\n",
       " '12963',\n",
       " '90733',\n",
       " '73561',\n",
       " '66889',\n",
       " '84097',\n",
       " '73863',\n",
       " '59218',\n",
       " '90208',\n",
       " '79641',\n",
       " '10330',\n",
       " '73087',\n",
       " '25511',\n",
       " '54405',\n",
       " '79081',\n",
       " '37905',\n",
       " '41290',\n",
       " '56935',\n",
       " '75752',\n",
       " '15011',\n",
       " '21282',\n",
       " '84148',\n",
       " '65214',\n",
       " '7170',\n",
       " '20539',\n",
       " '38053',\n",
       " '90501',\n",
       " '5057',\n",
       " '78085',\n",
       " '75398',\n",
       " '81791',\n",
       " '33515',\n",
       " '2132',\n",
       " '17192',\n",
       " '58625',\n",
       " '62372',\n",
       " '91146',\n",
       " '62674',\n",
       " '10106',\n",
       " '89645',\n",
       " '13462',\n",
       " '26523',\n",
       " '44934',\n",
       " '18107',\n",
       " '45152',\n",
       " '34809',\n",
       " '66897',\n",
       " '82924',\n",
       " '5304',\n",
       " '37547',\n",
       " '50489',\n",
       " '54758',\n",
       " '84401',\n",
       " '83636',\n",
       " '50854',\n",
       " '26391',\n",
       " '9954',\n",
       " '2949',\n",
       " '78515',\n",
       " '4512',\n",
       " '3057',\n",
       " '13435',\n",
       " '50656',\n",
       " '80658',\n",
       " '1741',\n",
       " '51356',\n",
       " '37561',\n",
       " '25240',\n",
       " '52895',\n",
       " '33347',\n",
       " '88927',\n",
       " '78334',\n",
       " '43848',\n",
       " '19442',\n",
       " '30354',\n",
       " '67934',\n",
       " '38338',\n",
       " '13493',\n",
       " '62870',\n",
       " '79963',\n",
       " '14684',\n",
       " '75260',\n",
       " '38261',\n",
       " '76857',\n",
       " '88152',\n",
       " '18918',\n",
       " '87037',\n",
       " '77781',\n",
       " '51839',\n",
       " '84995',\n",
       " '681',\n",
       " '43221',\n",
       " '78696',\n",
       " '72721',\n",
       " '12269',\n",
       " '44258',\n",
       " '71669',\n",
       " '89019',\n",
       " '33722',\n",
       " '13328',\n",
       " '17854',\n",
       " '54464',\n",
       " '24082',\n",
       " '12238',\n",
       " '10243',\n",
       " '59124',\n",
       " '9254',\n",
       " '30443',\n",
       " '48968',\n",
       " '77136',\n",
       " '78689',\n",
       " '26061',\n",
       " '65735',\n",
       " '83595',\n",
       " '18369',\n",
       " '38344',\n",
       " '76797',\n",
       " '38442',\n",
       " '7262',\n",
       " '89146',\n",
       " '86053',\n",
       " '27418',\n",
       " '72675',\n",
       " '42702',\n",
       " '33680',\n",
       " '55917',\n",
       " '73408',\n",
       " '65788',\n",
       " '20536',\n",
       " '44960',\n",
       " '53784',\n",
       " '33616',\n",
       " '90970',\n",
       " '51834',\n",
       " '70956',\n",
       " '67381',\n",
       " '22883',\n",
       " '36969',\n",
       " '40218',\n",
       " '69619',\n",
       " '69409',\n",
       " '43388',\n",
       " '2873',\n",
       " '83438',\n",
       " '51108',\n",
       " '75029',\n",
       " '53334',\n",
       " '22505',\n",
       " '32630',\n",
       " '72107',\n",
       " '74513',\n",
       " '29688',\n",
       " '84202',\n",
       " '88217',\n",
       " '85105',\n",
       " '67216',\n",
       " '23598',\n",
       " '54901',\n",
       " '78258',\n",
       " '22990',\n",
       " '48176',\n",
       " '21731',\n",
       " '62206',\n",
       " '90165',\n",
       " '60341',\n",
       " '48697',\n",
       " '23364',\n",
       " '61773',\n",
       " '37872',\n",
       " '64931',\n",
       " '81109',\n",
       " '28737',\n",
       " '18378',\n",
       " '88227',\n",
       " '61167',\n",
       " '52368',\n",
       " '45268',\n",
       " '87396',\n",
       " '54331',\n",
       " '86189',\n",
       " '87014',\n",
       " '10055',\n",
       " '72510',\n",
       " '57362',\n",
       " '30733',\n",
       " '28837',\n",
       " '25650',\n",
       " '38394',\n",
       " '57475',\n",
       " '83363',\n",
       " '88733',\n",
       " '56190',\n",
       " '68956',\n",
       " '74290',\n",
       " '55538',\n",
       " '55440',\n",
       " '46083',\n",
       " '86095',\n",
       " '86623',\n",
       " '29469',\n",
       " '43219',\n",
       " '32614',\n",
       " '31019',\n",
       " '41501',\n",
       " '28509',\n",
       " '69603',\n",
       " '51262',\n",
       " '16274',\n",
       " '69802',\n",
       " '79626',\n",
       " '12590',\n",
       " '27853',\n",
       " '37040',\n",
       " '89315',\n",
       " '59438',\n",
       " '19613',\n",
       " '44295',\n",
       " '68069',\n",
       " '56173',\n",
       " '75088',\n",
       " '10325',\n",
       " '82560',\n",
       " '13044',\n",
       " '12251',\n",
       " '87124',\n",
       " '21631',\n",
       " '74040',\n",
       " '19727',\n",
       " '50312',\n",
       " '53133',\n",
       " '40762',\n",
       " '21843',\n",
       " '67258',\n",
       " '3050',\n",
       " '43491',\n",
       " '13271',\n",
       " '72247',\n",
       " '65416',\n",
       " '89558',\n",
       " '37871',\n",
       " '64042',\n",
       " '54570',\n",
       " '4620',\n",
       " '89876',\n",
       " '64630',\n",
       " '67898',\n",
       " '57394',\n",
       " '8656',\n",
       " '22112',\n",
       " '22410',\n",
       " '50122',\n",
       " '10745',\n",
       " '14345',\n",
       " '29262',\n",
       " '40474',\n",
       " '62536',\n",
       " '28544',\n",
       " '21627',\n",
       " '72479',\n",
       " '48403',\n",
       " '55548',\n",
       " '82044',\n",
       " '1508',\n",
       " '3707',\n",
       " '5366',\n",
       " '70512',\n",
       " '55919',\n",
       " '29152',\n",
       " '57642',\n",
       " '7437',\n",
       " '4163',\n",
       " '72468',\n",
       " '43423',\n",
       " '1917',\n",
       " '72579',\n",
       " '30474',\n",
       " '79400',\n",
       " '87414',\n",
       " '19312',\n",
       " '22456',\n",
       " '25237',\n",
       " '47401',\n",
       " '64674',\n",
       " '11342',\n",
       " '32270',\n",
       " '43450',\n",
       " '84358',\n",
       " '50871',\n",
       " '58792',\n",
       " '12384',\n",
       " '62769',\n",
       " '46012',\n",
       " '40287',\n",
       " '52838',\n",
       " '60224',\n",
       " '63281',\n",
       " '82632',\n",
       " '36370',\n",
       " '18457',\n",
       " '1981',\n",
       " '21960',\n",
       " '22966',\n",
       " '15736',\n",
       " '33453',\n",
       " '51725',\n",
       " '50284',\n",
       " '41262',\n",
       " '28505',\n",
       " '58400',\n",
       " '36459',\n",
       " '48202',\n",
       " '61163',\n",
       " '68511',\n",
       " '6201',\n",
       " '30089',\n",
       " '65163',\n",
       " '40923',\n",
       " '85789',\n",
       " '13813',\n",
       " '22095',\n",
       " '5077',\n",
       " '33837',\n",
       " '1340',\n",
       " '88802',\n",
       " '52122',\n",
       " '53291',\n",
       " '16089',\n",
       " '34271',\n",
       " '53818',\n",
       " '59029',\n",
       " '32256',\n",
       " '52212',\n",
       " '53256',\n",
       " '10327',\n",
       " '20948',\n",
       " '20726',\n",
       " '86019',\n",
       " '36487',\n",
       " '82592',\n",
       " '70550',\n",
       " '59434',\n",
       " '78545',\n",
       " '10153',\n",
       " '1885',\n",
       " '66292',\n",
       " '11525',\n",
       " '91453',\n",
       " '24175',\n",
       " '89221',\n",
       " '17991',\n",
       " '74634',\n",
       " '40177',\n",
       " '22289',\n",
       " '4223',\n",
       " '79174',\n",
       " '48801',\n",
       " '72914',\n",
       " '32974',\n",
       " '55074',\n",
       " '644',\n",
       " '42693',\n",
       " '53711',\n",
       " '31363',\n",
       " '58793',\n",
       " '76805',\n",
       " '43131',\n",
       " '88596',\n",
       " '47764',\n",
       " '73636',\n",
       " '50625',\n",
       " '78602',\n",
       " '55244',\n",
       " '72690',\n",
       " '17008',\n",
       " '75037',\n",
       " '60868',\n",
       " '60061',\n",
       " '38780',\n",
       " '36613',\n",
       " '9190',\n",
       " '42977',\n",
       " '45190',\n",
       " '84909',\n",
       " '87351',\n",
       " '55768',\n",
       " '67625',\n",
       " '38595',\n",
       " '38381',\n",
       " '9453',\n",
       " '49755',\n",
       " '89563',\n",
       " '13887',\n",
       " '91377',\n",
       " '22885',\n",
       " '14358',\n",
       " '17190',\n",
       " '40933',\n",
       " '62671',\n",
       " '41116',\n",
       " '48097',\n",
       " '3975',\n",
       " '74572',\n",
       " '69521',\n",
       " '16498',\n",
       " '62285',\n",
       " '28877',\n",
       " '36563',\n",
       " '77757',\n",
       " '77392',\n",
       " '54204',\n",
       " '31169',\n",
       " '51983',\n",
       " '82999',\n",
       " '29090',\n",
       " '56056',\n",
       " '58072',\n",
       " '40062',\n",
       " '68235',\n",
       " '76776',\n",
       " '13231',\n",
       " '76047',\n",
       " '61675',\n",
       " '25291',\n",
       " '45295',\n",
       " '58601',\n",
       " '72506',\n",
       " '72383',\n",
       " '10878',\n",
       " '21997',\n",
       " '40565',\n",
       " '64024',\n",
       " '71690',\n",
       " '74201',\n",
       " '7005',\n",
       " '19240',\n",
       " '71007',\n",
       " '70999',\n",
       " '48637',\n",
       " '29323',\n",
       " '63313',\n",
       " '54723',\n",
       " '21757',\n",
       " '45925',\n",
       " '27145',\n",
       " '34524',\n",
       " '83379',\n",
       " '53088',\n",
       " '8035',\n",
       " '89599',\n",
       " '85036',\n",
       " '2879',\n",
       " '84108',\n",
       " '3996',\n",
       " '25475',\n",
       " '79773',\n",
       " '24831',\n",
       " '17143',\n",
       " '66684',\n",
       " '35811',\n",
       " '46431',\n",
       " '88407',\n",
       " '28882',\n",
       " '26546',\n",
       " '15551',\n",
       " '45036',\n",
       " '18558',\n",
       " '60066',\n",
       " '36737',\n",
       " '42485',\n",
       " '713',\n",
       " '72766',\n",
       " '69699',\n",
       " '35631',\n",
       " '80181',\n",
       " '56905',\n",
       " '58454',\n",
       " '17023',\n",
       " '46264',\n",
       " '3581',\n",
       " '656',\n",
       " '14664',\n",
       " '13600',\n",
       " '67227',\n",
       " '22490',\n",
       " '7588',\n",
       " '51080',\n",
       " '88598',\n",
       " '59631',\n",
       " '46182',\n",
       " '84373',\n",
       " '67495',\n",
       " '23135',\n",
       " '45214',\n",
       " '72958',\n",
       " '1208',\n",
       " '83863',\n",
       " '46517',\n",
       " '47425',\n",
       " '71991',\n",
       " '5948',\n",
       " '15636',\n",
       " '17848',\n",
       " '38710',\n",
       " '54010',\n",
       " '45379',\n",
       " '62344',\n",
       " '26500',\n",
       " '20797',\n",
       " '65315',\n",
       " '77761',\n",
       " '10144',\n",
       " '8991',\n",
       " '66096',\n",
       " '84822',\n",
       " '36355',\n",
       " '24335',\n",
       " '6387',\n",
       " '33029',\n",
       " '31536',\n",
       " '20862',\n",
       " '88003',\n",
       " '37579',\n",
       " '9031',\n",
       " '10442',\n",
       " '85670',\n",
       " '31940',\n",
       " '57082',\n",
       " '62695',\n",
       " '64819',\n",
       " '84580',\n",
       " '41757',\n",
       " '82644',\n",
       " '31714',\n",
       " '19973',\n",
       " '55299',\n",
       " '19907',\n",
       " '51454',\n",
       " '22656',\n",
       " '71785',\n",
       " '37003',\n",
       " '44946',\n",
       " '87908',\n",
       " '64273',\n",
       " '44221',\n",
       " '10808',\n",
       " '66258',\n",
       " '74477',\n",
       " '81643',\n",
       " '81696',\n",
       " '4346',\n",
       " '18338',\n",
       " '89080',\n",
       " '3821',\n",
       " '74055',\n",
       " '1644',\n",
       " '29493',\n",
       " '89931',\n",
       " '27707',\n",
       " '34569',\n",
       " '75548',\n",
       " '76944',\n",
       " '4995',\n",
       " '56735',\n",
       " '5731',\n",
       " '11471',\n",
       " '62826',\n",
       " '2053',\n",
       " '55171',\n",
       " '79196',\n",
       " '63982',\n",
       " '76117',\n",
       " '70918',\n",
       " '18802',\n",
       " '53983',\n",
       " '66257',\n",
       " '79183',\n",
       " '41832',\n",
       " '67753',\n",
       " '64997',\n",
       " '82062',\n",
       " '41982',\n",
       " '31962',\n",
       " '64038',\n",
       " '8789',\n",
       " '18075',\n",
       " '45260',\n",
       " '2736',\n",
       " '57249',\n",
       " '34345',\n",
       " '33895',\n",
       " '84696',\n",
       " '88723',\n",
       " '83161',\n",
       " '28417',\n",
       " '64909',\n",
       " '35950',\n",
       " '85427',\n",
       " '81604',\n",
       " '83120',\n",
       " '86390',\n",
       " '53517',\n",
       " '4694',\n",
       " '55305',\n",
       " '32919',\n",
       " '42432',\n",
       " '86523',\n",
       " '19574',\n",
       " '29983',\n",
       " '6023',\n",
       " '37319',\n",
       " '57998',\n",
       " '71040',\n",
       " '35386',\n",
       " '40973',\n",
       " '69704',\n",
       " '84924',\n",
       " '44783',\n",
       " '36053',\n",
       " '4732',\n",
       " '47060',\n",
       " '47119',\n",
       " '89134',\n",
       " '71843',\n",
       " '13656',\n",
       " '31175',\n",
       " '23485',\n",
       " '28962',\n",
       " '30974',\n",
       " '30582',\n",
       " '12583',\n",
       " '91350',\n",
       " '88527',\n",
       " '42722',\n",
       " '17391',\n",
       " '29110',\n",
       " '10272',\n",
       " '39932',\n",
       " '43187',\n",
       " '49380',\n",
       " '15164',\n",
       " '86824',\n",
       " '25383',\n",
       " '27694',\n",
       " '38631',\n",
       " '73974',\n",
       " '90708',\n",
       " '9304',\n",
       " '77472',\n",
       " '80647',\n",
       " '14319',\n",
       " '87303',\n",
       " '81202',\n",
       " '71611',\n",
       " '1977',\n",
       " '64958',\n",
       " '64209',\n",
       " '15397',\n",
       " '52360',\n",
       " '48254',\n",
       " '55535',\n",
       " '8177',\n",
       " '25756',\n",
       " '2246',\n",
       " '26269',\n",
       " '23746',\n",
       " '59403',\n",
       " '24583',\n",
       " '9663',\n",
       " '46904',\n",
       " '65969',\n",
       " '21693',\n",
       " '26897',\n",
       " '41374',\n",
       " '59509',\n",
       " '71895',\n",
       " '75055',\n",
       " '52171',\n",
       " '64064',\n",
       " '1851',\n",
       " '44146',\n",
       " '69342',\n",
       " '41973',\n",
       " '9580',\n",
       " '87260',\n",
       " '16610',\n",
       " '48620',\n",
       " '8416',\n",
       " '10825',\n",
       " '68367',\n",
       " '66406',\n",
       " '40894',\n",
       " '5686',\n",
       " '44745',\n",
       " '46290',\n",
       " '12376',\n",
       " '27593',\n",
       " '63106',\n",
       " '63033',\n",
       " '82911',\n",
       " '91138',\n",
       " '91545',\n",
       " '56167',\n",
       " '7295',\n",
       " '21566',\n",
       " '34439',\n",
       " '37289',\n",
       " '87438',\n",
       " '65880',\n",
       " '65982',\n",
       " '40596',\n",
       " '45165',\n",
       " '16393',\n",
       " '8380',\n",
       " '46199',\n",
       " '20605',\n",
       " '48165',\n",
       " '27406',\n",
       " '54663',\n",
       " '65353',\n",
       " '74116',\n",
       " '18704',\n",
       " '26965',\n",
       " '66863',\n",
       " '60778',\n",
       " '7079',\n",
       " '32048',\n",
       " '64019',\n",
       " '39495',\n",
       " '46376',\n",
       " '65154',\n",
       " '45187',\n",
       " '45066',\n",
       " '39717',\n",
       " '15554',\n",
       " '26798',\n",
       " '42301',\n",
       " '83445',\n",
       " '52893',\n",
       " '32389',\n",
       " '66374',\n",
       " '32708',\n",
       " '38718',\n",
       " '38811',\n",
       " '80139',\n",
       " '1186',\n",
       " '24226',\n",
       " '48903',\n",
       " '79301',\n",
       " '15253',\n",
       " '13253',\n",
       " '47093',\n",
       " '26385',\n",
       " '9317',\n",
       " '42598',\n",
       " '89434',\n",
       " '8850',\n",
       " '12033',\n",
       " '50398',\n",
       " '11505',\n",
       " '4695',\n",
       " '57872',\n",
       " '76791',\n",
       " '49756',\n",
       " '66060',\n",
       " '206',\n",
       " '29118',\n",
       " '10833',\n",
       " '79527',\n",
       " '89399',\n",
       " '85311',\n",
       " '3429',\n",
       " '45140',\n",
       " '2866',\n",
       " '39219',\n",
       " '51357',\n",
       " '77855',\n",
       " '1152',\n",
       " '10107',\n",
       " '83581',\n",
       " '6968',\n",
       " '45219',\n",
       " '58961',\n",
       " '15057',\n",
       " '63589',\n",
       " '41047',\n",
       " '14729',\n",
       " '27870',\n",
       " '91450',\n",
       " '7183',\n",
       " '28069',\n",
       " '48147',\n",
       " '57716',\n",
       " '53853',\n",
       " '17931',\n",
       " '12772',\n",
       " '30331',\n",
       " '87127',\n",
       " '59530',\n",
       " '5113',\n",
       " '8464',\n",
       " '44320',\n",
       " '27403',\n",
       " '47885',\n",
       " '87377',\n",
       " '43728',\n",
       " '18637',\n",
       " '85306',\n",
       " '75312',\n",
       " '9903',\n",
       " '56400',\n",
       " '7777',\n",
       " '40545',\n",
       " '67814',\n",
       " '45144',\n",
       " '44431',\n",
       " '7479',\n",
       " '28105',\n",
       " '43574',\n",
       " '41327',\n",
       " '64435',\n",
       " '44012',\n",
       " '27138',\n",
       " '27351',\n",
       " '57489',\n",
       " '11073',\n",
       " '36473',\n",
       " '54180',\n",
       " '18600',\n",
       " '15568',\n",
       " '22946',\n",
       " '22802',\n",
       " '1902',\n",
       " '56792',\n",
       " '66443',\n",
       " '80513',\n",
       " '13662',\n",
       " '11667',\n",
       " '31634',\n",
       " '9684',\n",
       " '87640',\n",
       " '61330',\n",
       " '42109',\n",
       " '63005',\n",
       " '77266',\n",
       " '79945',\n",
       " '29228',\n",
       " '69363',\n",
       " '39024',\n",
       " '50213',\n",
       " '54838',\n",
       " '66800',\n",
       " '87261',\n",
       " '90248',\n",
       " '50408',\n",
       " '15035',\n",
       " '53925',\n",
       " '34742',\n",
       " '42378',\n",
       " '60901',\n",
       " '20183',\n",
       " '82396',\n",
       " '23403',\n",
       " '28500',\n",
       " '79721',\n",
       " '64104',\n",
       " '14235',\n",
       " '57314',\n",
       " '66201',\n",
       " '58327',\n",
       " '69034',\n",
       " '19306',\n",
       " '70481',\n",
       " '40200',\n",
       " '79852',\n",
       " '5028',\n",
       " '27261',\n",
       " '38363',\n",
       " '41830',\n",
       " '47128',\n",
       " '53508',\n",
       " '66023',\n",
       " '15435',\n",
       " '39374',\n",
       " '34853',\n",
       " '86973',\n",
       " '1017',\n",
       " '15417',\n",
       " '5463',\n",
       " '88853',\n",
       " '17099',\n",
       " '3060',\n",
       " '77720',\n",
       " '33519',\n",
       " '84110',\n",
       " '67208',\n",
       " '35187',\n",
       " '13248',\n",
       " '42786',\n",
       " '56200',\n",
       " '61477',\n",
       " '72330',\n",
       " '29790',\n",
       " '49263',\n",
       " '67608',\n",
       " '1250',\n",
       " '30339',\n",
       " '33915',\n",
       " '50124',\n",
       " '36856',\n",
       " '78120',\n",
       " '37708',\n",
       " '27829',\n",
       " '10710',\n",
       " '42443',\n",
       " '72494',\n",
       " '75181',\n",
       " '54714',\n",
       " '88151',\n",
       " '16073',\n",
       " '20665',\n",
       " '42898',\n",
       " '59075',\n",
       " '77040',\n",
       " '16803',\n",
       " '21219',\n",
       " '40512',\n",
       " '33530',\n",
       " '37626',\n",
       " '78834',\n",
       " '11015',\n",
       " '15200',\n",
       " '32836',\n",
       " '52943',\n",
       " '21665',\n",
       " '82419',\n",
       " '57549',\n",
       " '11568',\n",
       " '41457',\n",
       " '38997',\n",
       " '20704',\n",
       " '1778',\n",
       " '28747',\n",
       " '77121',\n",
       " '12081',\n",
       " '11817',\n",
       " '22396',\n",
       " '62841',\n",
       " '62258',\n",
       " '7873',\n",
       " '68962',\n",
       " '35162',\n",
       " '36960',\n",
       " '42927',\n",
       " '75366',\n",
       " '8202',\n",
       " '773',\n",
       " '15604',\n",
       " '28642',\n",
       " '42520',\n",
       " '5162',\n",
       " '30093',\n",
       " '9719',\n",
       " '43868',\n",
       " '10972',\n",
       " '64079',\n",
       " '77381',\n",
       " '85204',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
