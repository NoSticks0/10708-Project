{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be here because by default Jupyter only adds the pwd to sys.path\n",
    "import os, sys\n",
    "if os.path.abspath('..') not in sys.path: sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pysrc.constants import cachepath, datapath, chartpath\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "with open(datapath(\"train.txt\")) as file:\n",
    "    for line in file:\n",
    "        (user_id, items) = line.split(maxsplit=1)\n",
    "        train_dict[user_id] = items\n",
    "\n",
    "with open(datapath(\"test.txt\")) as file:\n",
    "    for line in file:\n",
    "        split = line.split(maxsplit=1)\n",
    "        if (len(split) > 1):\n",
    "            (user_id, items) = split\n",
    "            test_dict[user_id] = items\n",
    "        else:\n",
    "            test_dict[split[0]] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for key in train_dict:\n",
    "    train.append([int(n) for n in train_dict[key].replace('\\n', '').split(' ')])\n",
    "\n",
    "for key in test_dict:\n",
    "    if len(test_dict[key]) == 0:\n",
    "        test.append([])\n",
    "    else:\n",
    "        test.append([int(n) for n in test_dict[key].replace('\\n', '').split(' ')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def log_likelihood_loss(y, yhat):\n",
    "    return -torch.mean(torch.sum(torch.log(yhat) * y, axis = 1))\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims = None,\n",
    "                 kl_weight = .2\n",
    "                 ):\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        \n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [512, 128]\n",
    "            \n",
    "        self.hidden_dims = hidden_dims\n",
    "            \n",
    "        modules.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        modules.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        modules.append(nn.LeakyReLU())\n",
    "        \n",
    "        # Build Encoder\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1])\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            modules.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(nn.Linear(hidden_dims[-1], input_dim),\n",
    "                                         nn.Sigmoid()\n",
    "                                        )\n",
    "        \n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, self.hidden_dims[0])\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        result = F.normalize(result, p=1)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), x, mu, log_var]\n",
    "\n",
    "    def loss_function(self, recons, x, mu, log_var) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        kld_weight = 20\n",
    "        recons_loss = log_likelihood_loss(x, recons)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dim = 91599\n",
    "lr = 1e-3\n",
    "model = VanillaVAE(input_dim = item_dim, latent_dim = 64, hidden_dims = [512, 128], kl_weight = 20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_ids = np.random.choice(np.array(list(train_dict.keys())).astype(int), int(.2*len(train_dict.keys())), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_uniform(data, p_relative = .1):\n",
    "    current_sparse = None\n",
    "    batch_size = 1000\n",
    "    i = 0\n",
    "    X = []\n",
    "    for i in data:\n",
    "        X.append(torch.zeros(item_dim).bool())\n",
    "        for item in i:\n",
    "            if np.random.random() > p_relative:\n",
    "                X[-1][item] = 1\n",
    "        X[-1] = X[-1]\n",
    "    X = torch.stack(X)\n",
    "    return X\n",
    "    \n",
    "def sparsify_items(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def sparsify_uers(data, m, sigma):\n",
    "    return\n",
    "\n",
    "def list_batch_to_ohe(data):\n",
    "    return sparsify_uniform(data, 0)\n",
    "\n",
    "dev_df = pd.Series(train).drop(test_ids)\n",
    "test_df = pd.Series(test).iloc[test_ids]\n",
    "\n",
    "train_df = dev_df.iloc[:int(.7*len(dev_df))]\n",
    "val_df = dev_df.iloc[int(.7*len(dev_df)):]\n",
    "\n",
    "train_tensor = list_batch_to_ohe(train_df)\n",
    "val_tensor = list_batch_to_ohe(val_df)\n",
    "test_tensor = list_batch_to_ohe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29480, 91599])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch 0\n",
      "tensor(859.6986, grad_fn=<AddBackward0>)\n",
      "tensor(416.7498, grad_fn=<AddBackward0>)\n",
      "tensor(416.3850, grad_fn=<AddBackward0>)\n",
      "tensor(870.5687, grad_fn=<AddBackward0>)\n",
      "tensor(1283.0436, grad_fn=<AddBackward0>)\n",
      "tensor(1696.3805, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(774.0750, grad_fn=<AddBackward0>)\n",
      "Train loss: 565.4601440429688\n",
      "Train r loss: 556.51904296875\n",
      "Train kl loss: -0.44705748558044434\n",
      "Val loss: 774.074951171875\n",
      "Val r loss: 772.856689453125\n",
      "Val kl loss: -0.06091318279504776\n",
      "Start Epoch 1\n",
      "tensor(675.5333, grad_fn=<AddBackward0>)\n",
      "tensor(504.4840, grad_fn=<AddBackward0>)\n",
      "tensor(501.8503, grad_fn=<AddBackward0>)\n",
      "tensor(684.7197, grad_fn=<AddBackward0>)\n",
      "tensor(1189.4111, grad_fn=<AddBackward0>)\n",
      "tensor(1690.3800, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(780.1489, grad_fn=<AddBackward0>)\n",
      "Train loss: 563.4600219726562\n",
      "Train r loss: 556.0555419921875\n",
      "Train kl loss: -0.37022408843040466\n",
      "Val loss: 780.14892578125\n",
      "Val r loss: 779.2476806640625\n",
      "Val kl loss: -0.04506218433380127\n",
      "Start Epoch 2\n",
      "tensor(450.3209, grad_fn=<AddBackward0>)\n",
      "tensor(663.3768, grad_fn=<AddBackward0>)\n",
      "tensor(564.7941, grad_fn=<AddBackward0>)\n",
      "tensor(447.4382, grad_fn=<AddBackward0>)\n",
      "tensor(1112.9962, grad_fn=<AddBackward0>)\n",
      "tensor(1681.9601, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(790.1403, grad_fn=<AddBackward0>)\n",
      "Train loss: 560.6533813476562\n",
      "Train r loss: 554.1364135742188\n",
      "Train kl loss: -0.32584941387176514\n",
      "Val loss: 790.1402587890625\n",
      "Val r loss: 789.413818359375\n",
      "Val kl loss: -0.03632130101323128\n",
      "Start Epoch 3\n",
      "tensor(684.4960, grad_fn=<AddBackward0>)\n",
      "tensor(405.7673, grad_fn=<AddBackward0>)\n",
      "tensor(582.6912, grad_fn=<AddBackward0>)\n",
      "tensor(687.7548, grad_fn=<AddBackward0>)\n",
      "tensor(1090.9364, grad_fn=<AddBackward0>)\n",
      "tensor(1671.9280, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(791.7340, grad_fn=<AddBackward0>)\n",
      "Train loss: 557.309326171875\n",
      "Train r loss: 550.318603515625\n",
      "Train kl loss: -0.34953320026397705\n",
      "Val loss: 791.7340087890625\n",
      "Val r loss: 791.0714111328125\n",
      "Val kl loss: -0.03313112258911133\n",
      "Start Epoch 4\n",
      "tensor(828.1009, grad_fn=<AddBackward0>)\n",
      "tensor(442.7463, grad_fn=<AddBackward0>)\n",
      "tensor(396.2893, grad_fn=<AddBackward0>)\n",
      "tensor(831.3463, grad_fn=<AddBackward0>)\n",
      "tensor(1271.4207, grad_fn=<AddBackward0>)\n",
      "tensor(1665.3020, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(812.4595, grad_fn=<AddBackward0>)\n",
      "Train loss: 555.1006469726562\n",
      "Train r loss: 547.1354370117188\n",
      "Train kl loss: -0.3982641398906708\n",
      "Val loss: 812.4595336914062\n",
      "Val r loss: 811.7470703125\n",
      "Val kl loss: -0.03562239184975624\n",
      "Start Epoch 5\n",
      "tensor(502.9187, grad_fn=<AddBackward0>)\n",
      "tensor(432.5183, grad_fn=<AddBackward0>)\n",
      "tensor(726.5701, grad_fn=<AddBackward0>)\n",
      "tensor(498.8038, grad_fn=<AddBackward0>)\n",
      "tensor(929.5796, grad_fn=<AddBackward0>)\n",
      "tensor(1667.4329, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(809.9119, grad_fn=<AddBackward0>)\n",
      "Train loss: 555.8109741210938\n",
      "Train r loss: 548.5175170898438\n",
      "Train kl loss: -0.36467257142066956\n",
      "Val loss: 809.9119262695312\n",
      "Val r loss: 809.3986206054688\n",
      "Val kl loss: -0.025664912536740303\n",
      "Start Epoch 6\n",
      "tensor(455.4496, grad_fn=<AddBackward0>)\n",
      "tensor(716.4788, grad_fn=<AddBackward0>)\n",
      "tensor(483.8147, grad_fn=<AddBackward0>)\n",
      "tensor(455.5928, grad_fn=<AddBackward0>)\n",
      "tensor(1179.8607, grad_fn=<AddBackward0>)\n",
      "tensor(1665.7927, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(811.2638, grad_fn=<AddBackward0>)\n",
      "Train loss: 555.2642211914062\n",
      "Train r loss: 548.7910766601562\n",
      "Train kl loss: -0.3236580789089203\n",
      "Val loss: 811.2637939453125\n",
      "Val r loss: 810.8111572265625\n",
      "Val kl loss: -0.02263079211115837\n",
      "Start Epoch 7\n",
      "tensor(465.4244, grad_fn=<AddBackward0>)\n",
      "tensor(696.6807, grad_fn=<AddBackward0>)\n",
      "tensor(477.0952, grad_fn=<AddBackward0>)\n",
      "tensor(465.7641, grad_fn=<AddBackward0>)\n",
      "tensor(1165.9280, grad_fn=<AddBackward0>)\n",
      "tensor(1646.4833, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(830.1479, grad_fn=<AddBackward0>)\n",
      "Train loss: 548.8277587890625\n",
      "Train r loss: 543.0764770507812\n",
      "Train kl loss: -0.2875603437423706\n",
      "Val loss: 830.14794921875\n",
      "Val r loss: 829.693115234375\n",
      "Val kl loss: -0.02274094521999359\n",
      "Start Epoch 8\n",
      "tensor(400.8564, grad_fn=<AddBackward0>)\n",
      "tensor(431.9857, grad_fn=<AddBackward0>)\n",
      "tensor(819.6979, grad_fn=<AddBackward0>)\n",
      "tensor(399.2641, grad_fn=<AddBackward0>)\n",
      "tensor(826.8071, grad_fn=<AddBackward0>)\n",
      "tensor(1654.5608, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(825.3441, grad_fn=<AddBackward0>)\n",
      "Train loss: 551.520263671875\n",
      "Train r loss: 546.1837768554688\n",
      "Train kl loss: -0.266824871301651\n",
      "Val loss: 825.3440551757812\n",
      "Val r loss: 824.8748779296875\n",
      "Val kl loss: -0.023458510637283325\n",
      "Start Epoch 9\n",
      "tensor(665.3775, grad_fn=<AddBackward0>)\n",
      "tensor(432.3381, grad_fn=<AddBackward0>)\n",
      "tensor(551.6883, grad_fn=<AddBackward0>)\n",
      "tensor(669.6481, grad_fn=<AddBackward0>)\n",
      "tensor(1100.5439, grad_fn=<AddBackward0>)\n",
      "tensor(1647.9043, grad_fn=<AddBackward0>)\n",
      "Val Eval 0\n",
      "tensor(844.5358, grad_fn=<AddBackward0>)\n",
      "Train loss: 549.3014526367188\n",
      "Train r loss: 544.1997680664062\n",
      "Train kl loss: -0.2550801932811737\n",
      "Val loss: 844.5357666015625\n",
      "Val r loss: 844.1025390625\n",
      "Val kl loss: -0.021661801263689995\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Start Epoch {epoch}\")\n",
    "    np.random.shuffle(train_df)\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loading_time = 0\n",
    "    for batch_X in train_dataloader:\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        train_loss = model.loss_function(recons, x, mu, log_var)['loss']\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    r_train_loss = 0\n",
    "    kl_train_loss = 0\n",
    "    train_batches = 0\n",
    "    for batch_X in train_dataloader:\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        loss = model.loss_function(recons, x, mu, log_var)\n",
    "        total_train_loss += loss['loss']\n",
    "        print(total_train_loss)\n",
    "        r_train_loss += loss['Reconstruction_Loss']\n",
    "        kl_train_loss += loss['KLD']\n",
    "        train_batches += 1\n",
    "        \n",
    "    total_train_loss /= train_batches\n",
    "    r_train_loss /= train_batches\n",
    "    kl_train_loss /= train_batches\n",
    "    \n",
    "    total_loss = 0\n",
    "    r_loss = 0\n",
    "    kl_loss = 0\n",
    "    batches = 0\n",
    "    for i in range(int(len(val_df)/batch_size)):\n",
    "        if i % 10==0:\n",
    "            print(\"Val Eval\", i)\n",
    "        batch_val = val_df[i * batch_size: min(len(train_df)-1, (i+1) * batch_size)]\n",
    "        batch_X = list_batch_to_ohe(batch_val)[0]\n",
    "        recons, x, mu, log_var = model.forward(batch_X)\n",
    "        loss = model.loss_function(recons, x, mu, log_var)\n",
    "        total_loss += loss['loss']\n",
    "        print(total_loss)\n",
    "        r_loss += loss['Reconstruction_Loss']\n",
    "        kl_loss += loss['KLD']\n",
    "        batches += 1\n",
    "    \n",
    "    total_loss /= batches\n",
    "    r_loss /= batches\n",
    "    kl_loss /= batches\n",
    "    \n",
    "    print(f'Train loss: {total_train_loss}')\n",
    "    print(f'Train r loss: {r_train_loss}')\n",
    "    print(f'Train kl loss: {kl_train_loss}')\n",
    "    \n",
    "    print(f'Val loss: {total_loss}')\n",
    "    print(f'Val r loss: {r_loss}')\n",
    "    print(f'Val kl loss: {kl_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = train_df[i * batch_size: min(len(train_df)-1, (i+1) * batch_size)]\n",
    "batch_X, load_time = list_batch_to_ohe(batch_train)\n",
    "total_loading_time += load_time\n",
    "recons, x, mu, log_var = model.forward(batch_X)\n",
    "train_loss = model.loss_function(recons, x, mu, log_var)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(-449.1120, grad_fn=<AddBackward0>),\n",
       " 'Reconstruction_Loss': tensor(-450.7499),\n",
       " 'KLD': tensor(-8.1898)}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_function(recons, x, mu, log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "pd.Series(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n"
     ]
    }
   ],
   "source": [
    "items = set()\n",
    "for key in train_dict:\n",
    "    if int(key) % 1000 == 0:\n",
    "        print(key)\n",
    "    \n",
    "    items = items.union(items, set(train_dict[key].replace('\\n', '').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91599"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'43979',\n",
       " '39065',\n",
       " '33205',\n",
       " '89792',\n",
       " '30523',\n",
       " '70033',\n",
       " '88878',\n",
       " '41347',\n",
       " '68344',\n",
       " '86949',\n",
       " '44103',\n",
       " '2951',\n",
       " '85171',\n",
       " '60160',\n",
       " '11377',\n",
       " '72298',\n",
       " '20399',\n",
       " '5311',\n",
       " '58242',\n",
       " '54536',\n",
       " '57125',\n",
       " '87479',\n",
       " '87537',\n",
       " '12518',\n",
       " '2327',\n",
       " '7306',\n",
       " '78055',\n",
       " '72185',\n",
       " '6584',\n",
       " '40171',\n",
       " '50978',\n",
       " '42484',\n",
       " '72646',\n",
       " '82726',\n",
       " '12963',\n",
       " '90733',\n",
       " '73561',\n",
       " '66889',\n",
       " '84097',\n",
       " '73863',\n",
       " '59218',\n",
       " '90208',\n",
       " '79641',\n",
       " '10330',\n",
       " '73087',\n",
       " '25511',\n",
       " '54405',\n",
       " '79081',\n",
       " '37905',\n",
       " '41290',\n",
       " '56935',\n",
       " '75752',\n",
       " '15011',\n",
       " '21282',\n",
       " '84148',\n",
       " '65214',\n",
       " '7170',\n",
       " '20539',\n",
       " '38053',\n",
       " '90501',\n",
       " '5057',\n",
       " '78085',\n",
       " '75398',\n",
       " '81791',\n",
       " '33515',\n",
       " '2132',\n",
       " '17192',\n",
       " '58625',\n",
       " '62372',\n",
       " '91146',\n",
       " '62674',\n",
       " '10106',\n",
       " '89645',\n",
       " '13462',\n",
       " '26523',\n",
       " '44934',\n",
       " '18107',\n",
       " '45152',\n",
       " '34809',\n",
       " '66897',\n",
       " '82924',\n",
       " '5304',\n",
       " '37547',\n",
       " '50489',\n",
       " '54758',\n",
       " '84401',\n",
       " '83636',\n",
       " '50854',\n",
       " '26391',\n",
       " '9954',\n",
       " '2949',\n",
       " '78515',\n",
       " '4512',\n",
       " '3057',\n",
       " '13435',\n",
       " '50656',\n",
       " '80658',\n",
       " '1741',\n",
       " '51356',\n",
       " '37561',\n",
       " '25240',\n",
       " '52895',\n",
       " '33347',\n",
       " '88927',\n",
       " '78334',\n",
       " '43848',\n",
       " '19442',\n",
       " '30354',\n",
       " '67934',\n",
       " '38338',\n",
       " '13493',\n",
       " '62870',\n",
       " '79963',\n",
       " '14684',\n",
       " '75260',\n",
       " '38261',\n",
       " '76857',\n",
       " '88152',\n",
       " '18918',\n",
       " '87037',\n",
       " '77781',\n",
       " '51839',\n",
       " '84995',\n",
       " '681',\n",
       " '43221',\n",
       " '78696',\n",
       " '72721',\n",
       " '12269',\n",
       " '44258',\n",
       " '71669',\n",
       " '89019',\n",
       " '33722',\n",
       " '13328',\n",
       " '17854',\n",
       " '54464',\n",
       " '24082',\n",
       " '12238',\n",
       " '10243',\n",
       " '59124',\n",
       " '9254',\n",
       " '30443',\n",
       " '48968',\n",
       " '77136',\n",
       " '78689',\n",
       " '26061',\n",
       " '65735',\n",
       " '83595',\n",
       " '18369',\n",
       " '38344',\n",
       " '76797',\n",
       " '38442',\n",
       " '7262',\n",
       " '89146',\n",
       " '86053',\n",
       " '27418',\n",
       " '72675',\n",
       " '42702',\n",
       " '33680',\n",
       " '55917',\n",
       " '73408',\n",
       " '65788',\n",
       " '20536',\n",
       " '44960',\n",
       " '53784',\n",
       " '33616',\n",
       " '90970',\n",
       " '51834',\n",
       " '70956',\n",
       " '67381',\n",
       " '22883',\n",
       " '36969',\n",
       " '40218',\n",
       " '69619',\n",
       " '69409',\n",
       " '43388',\n",
       " '2873',\n",
       " '83438',\n",
       " '51108',\n",
       " '75029',\n",
       " '53334',\n",
       " '22505',\n",
       " '32630',\n",
       " '72107',\n",
       " '74513',\n",
       " '29688',\n",
       " '84202',\n",
       " '88217',\n",
       " '85105',\n",
       " '67216',\n",
       " '23598',\n",
       " '54901',\n",
       " '78258',\n",
       " '22990',\n",
       " '48176',\n",
       " '21731',\n",
       " '62206',\n",
       " '90165',\n",
       " '60341',\n",
       " '48697',\n",
       " '23364',\n",
       " '61773',\n",
       " '37872',\n",
       " '64931',\n",
       " '81109',\n",
       " '28737',\n",
       " '18378',\n",
       " '88227',\n",
       " '61167',\n",
       " '52368',\n",
       " '45268',\n",
       " '87396',\n",
       " '54331',\n",
       " '86189',\n",
       " '87014',\n",
       " '10055',\n",
       " '72510',\n",
       " '57362',\n",
       " '30733',\n",
       " '28837',\n",
       " '25650',\n",
       " '38394',\n",
       " '57475',\n",
       " '83363',\n",
       " '88733',\n",
       " '56190',\n",
       " '68956',\n",
       " '74290',\n",
       " '55538',\n",
       " '55440',\n",
       " '46083',\n",
       " '86095',\n",
       " '86623',\n",
       " '29469',\n",
       " '43219',\n",
       " '32614',\n",
       " '31019',\n",
       " '41501',\n",
       " '28509',\n",
       " '69603',\n",
       " '51262',\n",
       " '16274',\n",
       " '69802',\n",
       " '79626',\n",
       " '12590',\n",
       " '27853',\n",
       " '37040',\n",
       " '89315',\n",
       " '59438',\n",
       " '19613',\n",
       " '44295',\n",
       " '68069',\n",
       " '56173',\n",
       " '75088',\n",
       " '10325',\n",
       " '82560',\n",
       " '13044',\n",
       " '12251',\n",
       " '87124',\n",
       " '21631',\n",
       " '74040',\n",
       " '19727',\n",
       " '50312',\n",
       " '53133',\n",
       " '40762',\n",
       " '21843',\n",
       " '67258',\n",
       " '3050',\n",
       " '43491',\n",
       " '13271',\n",
       " '72247',\n",
       " '65416',\n",
       " '89558',\n",
       " '37871',\n",
       " '64042',\n",
       " '54570',\n",
       " '4620',\n",
       " '89876',\n",
       " '64630',\n",
       " '67898',\n",
       " '57394',\n",
       " '8656',\n",
       " '22112',\n",
       " '22410',\n",
       " '50122',\n",
       " '10745',\n",
       " '14345',\n",
       " '29262',\n",
       " '40474',\n",
       " '62536',\n",
       " '28544',\n",
       " '21627',\n",
       " '72479',\n",
       " '48403',\n",
       " '55548',\n",
       " '82044',\n",
       " '1508',\n",
       " '3707',\n",
       " '5366',\n",
       " '70512',\n",
       " '55919',\n",
       " '29152',\n",
       " '57642',\n",
       " '7437',\n",
       " '4163',\n",
       " '72468',\n",
       " '43423',\n",
       " '1917',\n",
       " '72579',\n",
       " '30474',\n",
       " '79400',\n",
       " '87414',\n",
       " '19312',\n",
       " '22456',\n",
       " '25237',\n",
       " '47401',\n",
       " '64674',\n",
       " '11342',\n",
       " '32270',\n",
       " '43450',\n",
       " '84358',\n",
       " '50871',\n",
       " '58792',\n",
       " '12384',\n",
       " '62769',\n",
       " '46012',\n",
       " '40287',\n",
       " '52838',\n",
       " '60224',\n",
       " '63281',\n",
       " '82632',\n",
       " '36370',\n",
       " '18457',\n",
       " '1981',\n",
       " '21960',\n",
       " '22966',\n",
       " '15736',\n",
       " '33453',\n",
       " '51725',\n",
       " '50284',\n",
       " '41262',\n",
       " '28505',\n",
       " '58400',\n",
       " '36459',\n",
       " '48202',\n",
       " '61163',\n",
       " '68511',\n",
       " '6201',\n",
       " '30089',\n",
       " '65163',\n",
       " '40923',\n",
       " '85789',\n",
       " '13813',\n",
       " '22095',\n",
       " '5077',\n",
       " '33837',\n",
       " '1340',\n",
       " '88802',\n",
       " '52122',\n",
       " '53291',\n",
       " '16089',\n",
       " '34271',\n",
       " '53818',\n",
       " '59029',\n",
       " '32256',\n",
       " '52212',\n",
       " '53256',\n",
       " '10327',\n",
       " '20948',\n",
       " '20726',\n",
       " '86019',\n",
       " '36487',\n",
       " '82592',\n",
       " '70550',\n",
       " '59434',\n",
       " '78545',\n",
       " '10153',\n",
       " '1885',\n",
       " '66292',\n",
       " '11525',\n",
       " '91453',\n",
       " '24175',\n",
       " '89221',\n",
       " '17991',\n",
       " '74634',\n",
       " '40177',\n",
       " '22289',\n",
       " '4223',\n",
       " '79174',\n",
       " '48801',\n",
       " '72914',\n",
       " '32974',\n",
       " '55074',\n",
       " '644',\n",
       " '42693',\n",
       " '53711',\n",
       " '31363',\n",
       " '58793',\n",
       " '76805',\n",
       " '43131',\n",
       " '88596',\n",
       " '47764',\n",
       " '73636',\n",
       " '50625',\n",
       " '78602',\n",
       " '55244',\n",
       " '72690',\n",
       " '17008',\n",
       " '75037',\n",
       " '60868',\n",
       " '60061',\n",
       " '38780',\n",
       " '36613',\n",
       " '9190',\n",
       " '42977',\n",
       " '45190',\n",
       " '84909',\n",
       " '87351',\n",
       " '55768',\n",
       " '67625',\n",
       " '38595',\n",
       " '38381',\n",
       " '9453',\n",
       " '49755',\n",
       " '89563',\n",
       " '13887',\n",
       " '91377',\n",
       " '22885',\n",
       " '14358',\n",
       " '17190',\n",
       " '40933',\n",
       " '62671',\n",
       " '41116',\n",
       " '48097',\n",
       " '3975',\n",
       " '74572',\n",
       " '69521',\n",
       " '16498',\n",
       " '62285',\n",
       " '28877',\n",
       " '36563',\n",
       " '77757',\n",
       " '77392',\n",
       " '54204',\n",
       " '31169',\n",
       " '51983',\n",
       " '82999',\n",
       " '29090',\n",
       " '56056',\n",
       " '58072',\n",
       " '40062',\n",
       " '68235',\n",
       " '76776',\n",
       " '13231',\n",
       " '76047',\n",
       " '61675',\n",
       " '25291',\n",
       " '45295',\n",
       " '58601',\n",
       " '72506',\n",
       " '72383',\n",
       " '10878',\n",
       " '21997',\n",
       " '40565',\n",
       " '64024',\n",
       " '71690',\n",
       " '74201',\n",
       " '7005',\n",
       " '19240',\n",
       " '71007',\n",
       " '70999',\n",
       " '48637',\n",
       " '29323',\n",
       " '63313',\n",
       " '54723',\n",
       " '21757',\n",
       " '45925',\n",
       " '27145',\n",
       " '34524',\n",
       " '83379',\n",
       " '53088',\n",
       " '8035',\n",
       " '89599',\n",
       " '85036',\n",
       " '2879',\n",
       " '84108',\n",
       " '3996',\n",
       " '25475',\n",
       " '79773',\n",
       " '24831',\n",
       " '17143',\n",
       " '66684',\n",
       " '35811',\n",
       " '46431',\n",
       " '88407',\n",
       " '28882',\n",
       " '26546',\n",
       " '15551',\n",
       " '45036',\n",
       " '18558',\n",
       " '60066',\n",
       " '36737',\n",
       " '42485',\n",
       " '713',\n",
       " '72766',\n",
       " '69699',\n",
       " '35631',\n",
       " '80181',\n",
       " '56905',\n",
       " '58454',\n",
       " '17023',\n",
       " '46264',\n",
       " '3581',\n",
       " '656',\n",
       " '14664',\n",
       " '13600',\n",
       " '67227',\n",
       " '22490',\n",
       " '7588',\n",
       " '51080',\n",
       " '88598',\n",
       " '59631',\n",
       " '46182',\n",
       " '84373',\n",
       " '67495',\n",
       " '23135',\n",
       " '45214',\n",
       " '72958',\n",
       " '1208',\n",
       " '83863',\n",
       " '46517',\n",
       " '47425',\n",
       " '71991',\n",
       " '5948',\n",
       " '15636',\n",
       " '17848',\n",
       " '38710',\n",
       " '54010',\n",
       " '45379',\n",
       " '62344',\n",
       " '26500',\n",
       " '20797',\n",
       " '65315',\n",
       " '77761',\n",
       " '10144',\n",
       " '8991',\n",
       " '66096',\n",
       " '84822',\n",
       " '36355',\n",
       " '24335',\n",
       " '6387',\n",
       " '33029',\n",
       " '31536',\n",
       " '20862',\n",
       " '88003',\n",
       " '37579',\n",
       " '9031',\n",
       " '10442',\n",
       " '85670',\n",
       " '31940',\n",
       " '57082',\n",
       " '62695',\n",
       " '64819',\n",
       " '84580',\n",
       " '41757',\n",
       " '82644',\n",
       " '31714',\n",
       " '19973',\n",
       " '55299',\n",
       " '19907',\n",
       " '51454',\n",
       " '22656',\n",
       " '71785',\n",
       " '37003',\n",
       " '44946',\n",
       " '87908',\n",
       " '64273',\n",
       " '44221',\n",
       " '10808',\n",
       " '66258',\n",
       " '74477',\n",
       " '81643',\n",
       " '81696',\n",
       " '4346',\n",
       " '18338',\n",
       " '89080',\n",
       " '3821',\n",
       " '74055',\n",
       " '1644',\n",
       " '29493',\n",
       " '89931',\n",
       " '27707',\n",
       " '34569',\n",
       " '75548',\n",
       " '76944',\n",
       " '4995',\n",
       " '56735',\n",
       " '5731',\n",
       " '11471',\n",
       " '62826',\n",
       " '2053',\n",
       " '55171',\n",
       " '79196',\n",
       " '63982',\n",
       " '76117',\n",
       " '70918',\n",
       " '18802',\n",
       " '53983',\n",
       " '66257',\n",
       " '79183',\n",
       " '41832',\n",
       " '67753',\n",
       " '64997',\n",
       " '82062',\n",
       " '41982',\n",
       " '31962',\n",
       " '64038',\n",
       " '8789',\n",
       " '18075',\n",
       " '45260',\n",
       " '2736',\n",
       " '57249',\n",
       " '34345',\n",
       " '33895',\n",
       " '84696',\n",
       " '88723',\n",
       " '83161',\n",
       " '28417',\n",
       " '64909',\n",
       " '35950',\n",
       " '85427',\n",
       " '81604',\n",
       " '83120',\n",
       " '86390',\n",
       " '53517',\n",
       " '4694',\n",
       " '55305',\n",
       " '32919',\n",
       " '42432',\n",
       " '86523',\n",
       " '19574',\n",
       " '29983',\n",
       " '6023',\n",
       " '37319',\n",
       " '57998',\n",
       " '71040',\n",
       " '35386',\n",
       " '40973',\n",
       " '69704',\n",
       " '84924',\n",
       " '44783',\n",
       " '36053',\n",
       " '4732',\n",
       " '47060',\n",
       " '47119',\n",
       " '89134',\n",
       " '71843',\n",
       " '13656',\n",
       " '31175',\n",
       " '23485',\n",
       " '28962',\n",
       " '30974',\n",
       " '30582',\n",
       " '12583',\n",
       " '91350',\n",
       " '88527',\n",
       " '42722',\n",
       " '17391',\n",
       " '29110',\n",
       " '10272',\n",
       " '39932',\n",
       " '43187',\n",
       " '49380',\n",
       " '15164',\n",
       " '86824',\n",
       " '25383',\n",
       " '27694',\n",
       " '38631',\n",
       " '73974',\n",
       " '90708',\n",
       " '9304',\n",
       " '77472',\n",
       " '80647',\n",
       " '14319',\n",
       " '87303',\n",
       " '81202',\n",
       " '71611',\n",
       " '1977',\n",
       " '64958',\n",
       " '64209',\n",
       " '15397',\n",
       " '52360',\n",
       " '48254',\n",
       " '55535',\n",
       " '8177',\n",
       " '25756',\n",
       " '2246',\n",
       " '26269',\n",
       " '23746',\n",
       " '59403',\n",
       " '24583',\n",
       " '9663',\n",
       " '46904',\n",
       " '65969',\n",
       " '21693',\n",
       " '26897',\n",
       " '41374',\n",
       " '59509',\n",
       " '71895',\n",
       " '75055',\n",
       " '52171',\n",
       " '64064',\n",
       " '1851',\n",
       " '44146',\n",
       " '69342',\n",
       " '41973',\n",
       " '9580',\n",
       " '87260',\n",
       " '16610',\n",
       " '48620',\n",
       " '8416',\n",
       " '10825',\n",
       " '68367',\n",
       " '66406',\n",
       " '40894',\n",
       " '5686',\n",
       " '44745',\n",
       " '46290',\n",
       " '12376',\n",
       " '27593',\n",
       " '63106',\n",
       " '63033',\n",
       " '82911',\n",
       " '91138',\n",
       " '91545',\n",
       " '56167',\n",
       " '7295',\n",
       " '21566',\n",
       " '34439',\n",
       " '37289',\n",
       " '87438',\n",
       " '65880',\n",
       " '65982',\n",
       " '40596',\n",
       " '45165',\n",
       " '16393',\n",
       " '8380',\n",
       " '46199',\n",
       " '20605',\n",
       " '48165',\n",
       " '27406',\n",
       " '54663',\n",
       " '65353',\n",
       " '74116',\n",
       " '18704',\n",
       " '26965',\n",
       " '66863',\n",
       " '60778',\n",
       " '7079',\n",
       " '32048',\n",
       " '64019',\n",
       " '39495',\n",
       " '46376',\n",
       " '65154',\n",
       " '45187',\n",
       " '45066',\n",
       " '39717',\n",
       " '15554',\n",
       " '26798',\n",
       " '42301',\n",
       " '83445',\n",
       " '52893',\n",
       " '32389',\n",
       " '66374',\n",
       " '32708',\n",
       " '38718',\n",
       " '38811',\n",
       " '80139',\n",
       " '1186',\n",
       " '24226',\n",
       " '48903',\n",
       " '79301',\n",
       " '15253',\n",
       " '13253',\n",
       " '47093',\n",
       " '26385',\n",
       " '9317',\n",
       " '42598',\n",
       " '89434',\n",
       " '8850',\n",
       " '12033',\n",
       " '50398',\n",
       " '11505',\n",
       " '4695',\n",
       " '57872',\n",
       " '76791',\n",
       " '49756',\n",
       " '66060',\n",
       " '206',\n",
       " '29118',\n",
       " '10833',\n",
       " '79527',\n",
       " '89399',\n",
       " '85311',\n",
       " '3429',\n",
       " '45140',\n",
       " '2866',\n",
       " '39219',\n",
       " '51357',\n",
       " '77855',\n",
       " '1152',\n",
       " '10107',\n",
       " '83581',\n",
       " '6968',\n",
       " '45219',\n",
       " '58961',\n",
       " '15057',\n",
       " '63589',\n",
       " '41047',\n",
       " '14729',\n",
       " '27870',\n",
       " '91450',\n",
       " '7183',\n",
       " '28069',\n",
       " '48147',\n",
       " '57716',\n",
       " '53853',\n",
       " '17931',\n",
       " '12772',\n",
       " '30331',\n",
       " '87127',\n",
       " '59530',\n",
       " '5113',\n",
       " '8464',\n",
       " '44320',\n",
       " '27403',\n",
       " '47885',\n",
       " '87377',\n",
       " '43728',\n",
       " '18637',\n",
       " '85306',\n",
       " '75312',\n",
       " '9903',\n",
       " '56400',\n",
       " '7777',\n",
       " '40545',\n",
       " '67814',\n",
       " '45144',\n",
       " '44431',\n",
       " '7479',\n",
       " '28105',\n",
       " '43574',\n",
       " '41327',\n",
       " '64435',\n",
       " '44012',\n",
       " '27138',\n",
       " '27351',\n",
       " '57489',\n",
       " '11073',\n",
       " '36473',\n",
       " '54180',\n",
       " '18600',\n",
       " '15568',\n",
       " '22946',\n",
       " '22802',\n",
       " '1902',\n",
       " '56792',\n",
       " '66443',\n",
       " '80513',\n",
       " '13662',\n",
       " '11667',\n",
       " '31634',\n",
       " '9684',\n",
       " '87640',\n",
       " '61330',\n",
       " '42109',\n",
       " '63005',\n",
       " '77266',\n",
       " '79945',\n",
       " '29228',\n",
       " '69363',\n",
       " '39024',\n",
       " '50213',\n",
       " '54838',\n",
       " '66800',\n",
       " '87261',\n",
       " '90248',\n",
       " '50408',\n",
       " '15035',\n",
       " '53925',\n",
       " '34742',\n",
       " '42378',\n",
       " '60901',\n",
       " '20183',\n",
       " '82396',\n",
       " '23403',\n",
       " '28500',\n",
       " '79721',\n",
       " '64104',\n",
       " '14235',\n",
       " '57314',\n",
       " '66201',\n",
       " '58327',\n",
       " '69034',\n",
       " '19306',\n",
       " '70481',\n",
       " '40200',\n",
       " '79852',\n",
       " '5028',\n",
       " '27261',\n",
       " '38363',\n",
       " '41830',\n",
       " '47128',\n",
       " '53508',\n",
       " '66023',\n",
       " '15435',\n",
       " '39374',\n",
       " '34853',\n",
       " '86973',\n",
       " '1017',\n",
       " '15417',\n",
       " '5463',\n",
       " '88853',\n",
       " '17099',\n",
       " '3060',\n",
       " '77720',\n",
       " '33519',\n",
       " '84110',\n",
       " '67208',\n",
       " '35187',\n",
       " '13248',\n",
       " '42786',\n",
       " '56200',\n",
       " '61477',\n",
       " '72330',\n",
       " '29790',\n",
       " '49263',\n",
       " '67608',\n",
       " '1250',\n",
       " '30339',\n",
       " '33915',\n",
       " '50124',\n",
       " '36856',\n",
       " '78120',\n",
       " '37708',\n",
       " '27829',\n",
       " '10710',\n",
       " '42443',\n",
       " '72494',\n",
       " '75181',\n",
       " '54714',\n",
       " '88151',\n",
       " '16073',\n",
       " '20665',\n",
       " '42898',\n",
       " '59075',\n",
       " '77040',\n",
       " '16803',\n",
       " '21219',\n",
       " '40512',\n",
       " '33530',\n",
       " '37626',\n",
       " '78834',\n",
       " '11015',\n",
       " '15200',\n",
       " '32836',\n",
       " '52943',\n",
       " '21665',\n",
       " '82419',\n",
       " '57549',\n",
       " '11568',\n",
       " '41457',\n",
       " '38997',\n",
       " '20704',\n",
       " '1778',\n",
       " '28747',\n",
       " '77121',\n",
       " '12081',\n",
       " '11817',\n",
       " '22396',\n",
       " '62841',\n",
       " '62258',\n",
       " '7873',\n",
       " '68962',\n",
       " '35162',\n",
       " '36960',\n",
       " '42927',\n",
       " '75366',\n",
       " '8202',\n",
       " '773',\n",
       " '15604',\n",
       " '28642',\n",
       " '42520',\n",
       " '5162',\n",
       " '30093',\n",
       " '9719',\n",
       " '43868',\n",
       " '10972',\n",
       " '64079',\n",
       " '77381',\n",
       " '85204',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
